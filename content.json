{"meta":{"title":"金天的个人博客","subtitle":null,"description":"你永远不知道会有什么不幸的事情发生，停顿一秒，忽视它，继续前行","author":"Jintian","url":"http://yoursite.com"},"pages":[{"title":"About","date":"2020-01-03T02:27:43.775Z","updated":"2020-01-03T02:27:43.775Z","comments":true,"path":"about/index copy.html","permalink":"http://yoursite.com/about/index copy.html","excerpt":"","text":"简介金天，中南大学控制科学与工程硕士在读，曾在宾夕法尼亚大学担任过一年访问学者。主要研究领域为计算机视觉目标检测与跟踪、基于学习的自然语言处理智能等。在腾讯自动驾驶实验室、滴滴自动驾驶部门均有过夯实的工作阅历。发表过人工智能领域内期刊数篇，在凹优化问题上提出过使用进化算法优化神经网络参数的理论。常年活跃于各大社交媒体，曾受邀加入百度PaddlePaddle部门文章撰稿人。在GitHub上开源了多个上千个star项目，备受好评。 联系 知乎：金天 GitHub: jinfagang 知乎专栏： 人工智能从入门到逆天杀神 、 每周一项黑科技(TrackTech) 博客： https://jinfagang.github.io 个人作品： 萝莉萝莉"},{"title":"About","date":"2020-01-03T02:27:43.775Z","updated":"2020-01-03T02:27:43.775Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"金天中南大学在读研究生，控制科学与工程硕士，在美国宾夕法尼亚大学担任过一年访问学者。前腾讯自动驾驶部门研究员，研究领域为深度学习，泛人工智能，NLP，CV等方向。发表过人工智能领域内期刊数篇，在凹优化问题上提出过使用进化算法优化神经网络参数的理论。常年活跃于各大社交媒体，曾受邀加入百度PaddlePaddle部门文章撰稿人。在GitHub上开源了多个上千个star项目，备受好评。现从事人工智能教学工作，并组织创办AI-Man的直播新媒体人工智能教授平台，以VR、实时Coding等现代化的技术为广大学习者提供完全免费的视频课程教学。立志于将高科技以零姿态教授给他人。热爱人工智能，以及电子硬件制作，是要成为钢铁侠的男人欧曼信息科技有限公司CEO，业余魔术师Contact知乎：金天GitHub: jinfagang WeChat: jintianandmerry 微博：大魔术师金天"},{"title":"Achievements","date":"2020-01-03T02:27:43.775Z","updated":"2020-01-03T02:27:43.775Z","comments":true,"path":"achievements/index.html","permalink":"http://yoursite.com/achievements/index.html","excerpt":"","text":"Welcome to my Achievements Wall Here I will show some interesting projects and my personal achievements No.5 Github OpenSource Project Trending Top 3 I place this into 5th position, because it is not good enough, I created it but not keep on that,this project - weibo_terminater, now on Githubhas 1499 stars, and so many people add my wechat to talk about their life :). It’s really interesting. No.4 Jarvis - Artificial Intelligence Assistant Ever HaveJarvis maybe that most significant things I have done. I finish build Jarvis on 2017-4-28, which takes me almost 5 weeks. I’d like to paste some chat screen shots. And you can find Jarvis via my wechat, Jarvis now have more important mission to do rather than gossip. The point is that, Jarvis can specific images!!"},{"title":"Resume","date":"2020-01-03T02:27:43.776Z","updated":"2020-01-03T02:27:43.776Z","comments":true,"path":"resume/index.html","permalink":"http://yoursite.com/resume/index.html","excerpt":"","text":"联系方式 手机：15116123160 (随时可以呼叫) Email：nicholasjela@gmail.com/jinfagang19@163.com QQ/微信号：1195889656/jintianiloveu 地址：长沙岳麓区中南大学校本部13舍515 个人信息 金发岗/男/1993 硕士/控制科学与工程学硕 工作年限：2018应届毕业生 技术博客：http://jinfagang.github.io Github：http://github.com/jinfagang 期望职位：深度学习相关岗位，计算机视觉深度学习岗，自然语言处理深度学习岗均可 期望城市：杭州/深圳/广州 工作经历腾讯(Intern)(2017年6月~2017年7月)自动驾驶部门目标检测与跟踪我负责并实现了： 实现了基于强化学习的跟踪算法，并搭建了强化学习训练框架，使用了DDPG算法训练，利用RNN和帧与帧直接的时间序列关系对检测框进行评估，只使用检测算法在fastrcnn上IOU大于0.7的评测下准确率只有85%，使用该框架后大大减少了漏检和误检情况，模型准确率可以在IOU大于0.8的情况下达到87.5%； 同时结合卡尔曼滤波进行帧间预测，实现了传统算法层面的跟踪算法，基于deepsort将badcase消减了近20%； 在此期间还基于Python3和PyQT5实现并维护了一个标注工具sloth_autolab。 滴滴出行(Intern) (2016年11月 ~ 2017年2月)无人驾驶车目标检测benchmark我在此项目负责了： 修改目标检测算法中state-of-art的yolo算法，训练kitti中Car，Pedestrain，Bicylce三个类别的模型，并获取mAP，与同组其他算法做对比，yolo的在该数据集上的Top3的Recall约为0.3，Precision约为0.72，效果略差于mscnn，ssd，但速度是最快的(2017-4 再次跑yolo-v2，结果无论是在mAP还是主观上都比ssd要准确，依旧很快速) 协助同组发布benchmark网站，该部分工作善尾。 无人驾驶车目标检测使用caffe工业级实现交通信号灯检测与识别我在负责整个检测系统的该部分实现，实现了一下工作: 基于SSD实现了红，绿，黄信号灯的检测，在位置检测上，SSD算法在召回率上偏高，位置检测不准导致很多漏检，最终通过降低SSD算法内部检测框输入阈值解决； 使用caffe进行模型融合，在已经实现交通信号等检测的系统之上，与同组其他同事融合Car，Pdestrain，Cyclist模型，最终实现实时工业级检测无人车使用场景下的物体检测(在我返校之后应该还有物体depth检测工作没有完成，不过这是另外一组负责stero的同事做的工作)。 开源项目和作品开源项目show code directly. tensorflow_poems：人工智能作诗机器人，这个作品是NLP的一个尝试，因为自己研究领域是时间序列处理，因此对于RNN，LSTM等模型的应用也不在话下，该项目目前(2017-5) star数为 400， fork数为127. pytorch_cycle_gan：该项目是生成对抗模型GAN的一个尝试，是对今年新出论文 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks 原始torch代码的一个重构，我在这个开源项目中实现了单张图片的预测，修改了一些opt，最终实现效果是单张图片通过CycleGAN生成对应的图片，比如白天的风景照转换成同照片夜晚下的效果，几乎实时. weibo_terminator：该项目实现的是一个微博爬虫系统，没有使用任何第三方框架，多线程实现，任务调度等全手工实现，这个项目在我的所有项目中得到的关注度最高，目前有 1576 个star，开源当天就上了GitHub Python Trending排行榜世界第四，目前累积聚集了约200多位开发者共同进行微博的爬取工作。该项目的目的是发动一次数据众筹，获取的语料用来训练深度学习对话模型，以及微博情感分析，舆论风控等应用。截止目前，我们总共爬取了约1.2G珍贵的中文对话语料。 pytorch_chatbot：基于当下流行的seq2seq以及attention machanism，实现的聊天对话机器人，训练的语料就是我们收集的微博对话语料，具体效果可以见GitHub截图，目前已经基于该模型实现一个微信聊天机器人Jarvis，他的微信号: jintianandmerry，我在的时候机器人都在线. 发表的论文 工业智能和数据驱动的卷烟烘丝入口含水率的预测与应用 CPCC中国过程控制会议收录(EI) Adversarial Generate Sequencial Network On Time Series Prediction (On Process) 参与的比赛滴滴出行算法大赛 (2016年6月 ~ 2016年7月)比赛队伍：2000支，4000多人比赛成绩：Top20比赛任务：基于滴滴出行海量用户出行数据, 根据某城市每天每个时间片段的天气, 交通拥堵情况, 城市地理位置等复杂因素来准备预测下一时间段的供需缺口, 带领团队获得top50的成绩, 预测误差为0.26, 成功晋级决赛。比赛完成的任务: 对raw数据进行预处理，将所有特征导入到同一个csv文件中，对一天中产生的所有数据按照切片间隔进行切片; 进行特征工程的特征选取，选取三个时间段内的均值，标准差，最大值最小值等参数作为特征，除此之外还对节假日，周末这些不可量化的特征进行哑变量表示，并对日期进行one-hot编码，总共组成了近300多维附加特征。 上海BOT计算机视觉大赛 (2016年9月 ~ 2016年10月)比赛队伍：1500支，3600多人比赛成绩：Top50比赛任务：基于50多万幅12种动物图片进行深度学习网络建模并进行识别, 图像不仅大小不一, 而且有很多非实体图像, 难度空前, 采用改进的vgg深度学习网络并对图像预先进行降噪处理将分类精度提高至98%.比赛完成的任务: 负责基于keras的卷积神经网络搭建; 对图片进行包括随机裁剪，翻转，对称，噪声化等预处理，使样本集增大了10%; 对基于keras的VGG分类网络进行调试，对VGG-16最后softmax层进行重构，使之满足12类的分类要求。 技能清单个人技能清单，以下均为我熟练使用的技能(核心技能)： 语言： Python(95%)/Java(40%)/C++(30%)/ 深度框架: Tensorflow/Tensorflow-Lit/Pytorch/Caffe(2)/ 数据建模：Python/Scikit-Learn/Numpy/Pandas 图像检测：Yolo-v2/SSD/Fast-rcnn NLP：gensim/NLTK/seq2seq-attention/QA-System/Sentiment-Analysis/ChatBot 前沿理论: GAN/Style-Transfer/Reinfocement-Learning 周边技能清单(业余爱好)： 数据库：PostgresSQL/MongoDB 移动端：Android/Java 版本管理、文档和自动化部署工具：Git/MarkDown 云：Django/Flask 致谢感谢您花时间阅读我的简历，期待能有机会和您共事。"},{"title":"分类","date":"2016-10-25T11:28:20.000Z","updated":"2020-01-03T02:27:43.776Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2016-10-25T11:28:20.000Z","updated":"2020-01-03T02:27:43.776Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2020_01_02_04_深度学习编译器概述一","slug":"2020_01_02_04_深度学习编译器概述一","date":"2020-01-02T08:13:13.000Z","updated":"2020-02-17T06:01:58.935Z","comments":true,"path":"2020/01/02/2020_01_02_04_深度学习编译器概述一/","link":"","permalink":"http://yoursite.com/2020/01/02/2020_01_02_04_深度学习编译器概述一/","excerpt":"本文介绍 2020_01_0204深度学习编译器概述一","text":"本文介绍 2020_01_0204深度学习编译器概述一 2020_01_0204深度学习编译器概述一 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 最近在学习一些新的知识，发现一个很有趣的主题，那就是深度学习编译器。先贴出一个网站：https://tvm.d2l.ai/ 。本系列文章不是翻译，也不是复制粘贴，而是结合自己的思路的一些介绍，主要目的就这么几个： 常用模型的优化以及他们涉及到的算子如何设计，比如resnet, mobilenet, yolov3, bert等； 将常用模型部署于任何平台，比如ARM，CPU，RK3399, RaspberryPI，AMD，mobile-GPUs等。 如何实现他们，以及为什么要实现他们。 那么接下来需要引入的问题就是，什么是深度学习编译器？我们为什么需要深度学习编译器？本文也是按照李沐大佬的google doc来进行编写，会残杂一些自己的思考，大家也可以直接一遍阅读原文，一遍阅读本文：https://docs.google.com/document/d/14Bgo9TgczROlqcTinS5-Y4hyig-ae0Rm8uIFQl9OAEA/edit ## 引子 深度学习技术已经发展到了一定的地步，以至于我们已经到了完完全全的部署更上一层楼阶段，简单的来说，我们希望拿着这些足够好的模型，让它以非常快的速度跑在我们需要的平台上，为了达到这些目的，很多厂商也推出了很多自己的套件，比如英伟达的TensorRT，比如Intel的OpenVINO，但说实话，不同的平台需要你自己去fit它的SDK，可以说工作量还是很大的，就比如你想用TensorRT加速，那么几乎需要将代码用TensorRT重写一遍，除了最中间的网络推理部分。此时深度学习编译器出来了，比较典型的就是tvm，tvm做什么事情呢？它会把你模型的算子，自动调用他们内部的IR去针对性的针对不同平台进行优化，所谓的优化，不仅仅是类似于TensorRT或者OpenVINO这样的算子层面的优化，还包括一些图的逻辑上的优化，简单来说它带来了两个好处： 你的模型精度牛逼，但是优化不一定是最好的； 你不需要再为每一个平台去写部署代码了。 理想很丰满，但是带来的问题也很多。这也是我在很久之前就diss的一个地方： 你可以测CPU下的优化，确实比tensorflow或者mxnet原生的CPU速度来的快，但是比不上openvino； 你可以测GPU下的tvm优化，但是几乎可以100%确定，比不上tensorrt。 说白了就是，你做一个跨平台的东西，表面上很厉害很牛逼，但是是否确实如此，能搞得过人家硬件厂商自身吗？不好说。但，深度学习编译器真的就不香吗，如果你想要得到跨平台的好处，那么还是不得不牺牲一些东西了。 接下来我想对比两个问题，tvm在CPU上的加速表现，tvm在GPU上的加速表现 以及 tvm本身在FPGA上的支持程度如何？ 要解决这两个问题首先得拆解一下tvm的代码了。在这之前，先了解一下道听途说的tvm内部包含的优化方式： 利用专家知识来优化模型； 在硬件上跑多个trial来测试硬件的最佳实现； 针对不同的算子（op），在不同的硬件平台上实现不同的优化方案，使其在硬件平台上获得最优。 即便如此，目前的tvm还存在两个问题：很依赖与mxnet，对于pytorch是否友好？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"2020_01_02_01_2020的小目标","slug":"2020_01_02_01_2020的小目标","date":"2020-01-02T05:07:07.000Z","updated":"2020-02-17T06:02:11.145Z","comments":true,"path":"2020/01/02/2020_01_02_01_2020的小目标/","link":"","permalink":"http://yoursite.com/2020/01/02/2020_01_02_01_2020的小目标/","excerpt":"本文介绍 2020_01_02_01_2020的小目标","text":"本文介绍 2020_01_02_01_2020的小目标 2020_01_02_01_2020的小目标 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 制定一个2020的小目标计划，到2021年的时候再回过头来看。 2019总结： 1). 完成了工作上的迁移，还需要探索更深入的领域，同时借助弯道进行快速的超车； 2). 经济上基本上完成了第一个阶段的任务，生活上完成了基本的基础设施建设，这部分还需要更进一步进行本质的突破，力求2020的生活更加方便快捷； 3). 副业完成了小规模突破，2020需要开拓巩固现有优势，持续在新的领域发力，形成多方掎角之势进行多方位布局，为经济收入提供多个引擎。 2020规划： 2020将是极其重要的一年，它的成绩决定了未来五年的发展方向和态势，需要更进一步的开拓看似不可能的渠道和领域。这一年需要完成的任务包括： 年初完成新车购置，不惜一切代价完成这一任务； 在第一季度完成个人事业的一个技术提升，比如转变方向、提升一个层次等，并开拓一个能够提供流量的副业，如视频拍摄录制等； 第一季度末尾上线一些个人的vlog作品，寻找到自己的方向，并切入流量领域； 神力平台在第一季度完成升级，引入APP社区体制，力求完成第一季度注册用户数突破2000人，会员转化率超过50%，实现除会员收入外新的盈利模式； 个人能力提升计划，要么选择研究院跳板方式争取读博机会，要么开始计划性的读MBA博士学位，学位提升计划将会是2020的重中之重。 年底之前完成房款首付筹备工作，不惜一切代价完成，次年开春开始计划性的上车。 沉舟侧畔千帆过，病树前头万木春。砥砺前行，披荆斩棘！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Detectron2训练自己的实例分割数据集","slug":"2019_10_22_14_Detectron2训练自己的实例分割数据集","date":"2019-10-22T06:37:35.000Z","updated":"2020-02-17T06:01:58.935Z","comments":true,"path":"2019/10/22/2019_10_22_14_Detectron2训练自己的实例分割数据集/","link":"","permalink":"http://yoursite.com/2019/10/22/2019_10_22_14_Detectron2训练自己的实例分割数据集/","excerpt":"本文介绍 Detectron2训练自己的实例分割数据集","text":"本文介绍 Detectron2训练自己的实例分割数据集 Detectron2训练自己的实例分割数据集 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 本文介绍如何构造自己的类coco数据集，并用detectron2来训练并预测。实际上detectron2出来已经有几天了。但这个框架个人感觉离真正工业使用还有点距离，首先第一点是不好部署，其次是相关的其他模型导出支持比较差，比如不支持onnx，同时即便是导出的onnx模型也很难用上一些加速框架进行加速，比如不支持TensorRT。但如果你不是追求速度，用python做推理也是可以的，并且最关键的是，你可以用你的数据集训练你的模型，或者是在这个框架上增加一些自己的修改，看看效果是不是更好之类。 首先看看这个如何来训练自己的数据集。我们今天要用的数据是这个： wget https://github.com/Tony607/detectron2_instance_segmentation_demo/releases/download/V0.1/data.zip 这篇文章很大借鉴于原作者的一些尝试，感兴趣的朋友可以给他github一个star，人家也不容易。这个data是一个非常非常适合来测试分割的一个微型数据集，小到什么程度？只有那么25张图片。。 类别大概是: cats: [&#123;&apos;supercategory&apos;: &apos;date&apos;, &apos;id&apos;: 1, &apos;name&apos;: &apos;date&apos;&#125;, &#123;&apos;supercategory&apos;: &apos;fig&apos;, &apos;id&apos;: 2, &apos;name&apos;: &apos;fig&apos;&#125;, &#123;&apos;supercategory&apos;: &apos;hazelnut&apos;, &apos;id&apos;: 3, &apos;name&apos;: &apos;hazelnut&apos;&#125;] （这里date的意思是枣椰子，fig的意思是无花果，hazelnut是榛子。) 如果你下载好了数据，那么基本上我们可以开始了。大家可以看到这个数据集还有一个trainval.json，完全是按照coco的标注方式来标注的。coco的json格式也是目前比较通用的是instance segmentation 或者是Panoptic segmentation标注格式。 Setup Detectron2关于如何安装detectron2这里不展开叙述，大家可以按照github给予的步骤来。这里给大家几点提醒： 要python3.6; 要pytorch1.3. 其他的没有了。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"ONNXRuntime采用TensorRT后端快速推理","slug":"2019_10_08_15_ONNXRuntime采用TensorRT后端快速推理","date":"2019-10-08T07:02:28.000Z","updated":"2020-02-17T06:01:58.935Z","comments":true,"path":"2019/10/08/2019_10_08_15_ONNXRuntime采用TensorRT后端快速推理/","link":"","permalink":"http://yoursite.com/2019/10/08/2019_10_08_15_ONNXRuntime采用TensorRT后端快速推理/","excerpt":"本文介绍 ONNXRuntime采用TensorRT后端快速推理","text":"本文介绍 ONNXRuntime采用TensorRT后端快速推理 ONNXRuntime采用TensorRT后端快速推理 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu onnxruntime是一个专门推理onnx模型的前向推理框架，它有一个好处就是模型统一、接口统一但是加速的框架不同。比如你如果在GPU上做推理，那么就用GPU后端，要更快一点可以切换到TensorRT后端，如果你在CPU上做推理，那么就采用openvino来做后端。 由于这些第三方加速很专业，比如TensorRT，比如OpenVINO，onnxruntime的聪明之处在于，集总价之长与一身。但是这样也带来了一些问题，那就是并非所有的模型都能很好的支持，比如CPU支持的op应该是最多的，有一些层TensorRT不支持，这种情况下onnxruntime会自动把不支持的操作，转到CUDA上来，这样就很方便了。其实onnruntime推理，也很简单，你只需要写一次代码，就可以在不同的平台上以最快的速度运行。我们接下来尝试使用onnxruntime来加速一下maskrcnn的onnx模型。 目前来说，有两个maskrcnn模型用的比较多，一个是maskrcnn-benmark, 另外一个是mmdetection, 这两个框架都可以将maskrcnn模型导出到onnx。我们将采用onnxruntime，来尝试推理，在推理之前，我们的思路是，先用CPU后端，然后用TensorRT后端。这样看看CPU结果是否正确，看看是否支持Tensorrt，但是大概率，是不支持tensorrt，至少有个别层是不支持的，这个时候就需要采用onnxruntime的fallback策略，将不支持的操作自动fallback到CUDA上。 在写这篇文章的时候，onnxruntime刚刚增加了TensorRT6.0的支持，这使得我们有可能对一些动态输入的模型在tensorrt中得到支持。比如我们要测试的maskrcnn。 经测试，最新的onnxruntime可以在GPU上推理maskrcnn的onnx模型。可以调用TensorRT的op，但是貌似，有一些不支持的自动在CPU上执行了，导致运行速度很慢。一些op比如：Gather, Concate 等还是在CPU上跑。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"ONNX里面的元运算","slug":"2019_09_27_15_ONNX里面的元运算","date":"2019-09-27T07:56:38.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/09/27/2019_09_27_15_ONNX里面的元运算/","link":"","permalink":"http://yoursite.com/2019/09/27/2019_09_27_15_ONNX里面的元运算/","excerpt":"本文介绍 ONNX里面的元运算","text":"本文介绍 ONNX里面的元运算 ONNX里面的元运算 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 其实这个有点标题党，但这篇文章要做的事情，还是很有意义的，我们很多时候再用onnx转模型的时候，遇到的都不是一个层的问题，而往往是某个node不对，要么缺少attribute，要么你的后端不支持，那么这个node到底是如何运算的你就需要心知肚明了，就好像这个node是你写的一样，这个我就叫做unit onnx, 将一个大的东西拆解，拆解到内裤都不剩，看看这里面到底卖的什么药。 onnx元运算第一步做什么？简单，我们拆解一个很简单的node： Slice. 元运算先看一下Slice在onnx标准中的定义： Produces a slice of the input tensor along multiple axes. Similar to numpy: 说白了，就是根据你指定的idx选择不同的目标，用numpy来表示就是： &gt;&gt;&gt; x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; x[1:7:2]array([1, 3, 5]) 这个Node可以说非常简单了，但是如果是你，你会怎么来实现呢？先不着急实现，在不同的onnx opset中，对Slice的定义其实是有区别的： opset 10: opset10中的slice inputs要求3-5 data : T Tensor of data to extract slices from. starts : Tind 1-D tensor of starting indices of corresponding axis in axes ends : Tind 1-D tensor of ending indices (exclusive) of corresponding axis in axes axes (optional) : Tind 1-D tensor of axes that starts and ends apply to. steps (optional) : Tind 1-D tensor of slice step of corresponding axis in axes. Default to 1. opset9: opset10以前版本中的slice，还有attribute属性： attritbute: axes : list of ints Axes that starts and ends apply to. It’s optional. If not present, will be treated as [0, 1, …, len(starts) - 1]. ends : list of ints (required) Ending indices (exclusive) of corresponding axis in axes` starts : list of ints (required) Starting indices of corresponding axis in axes inputs: data : T Tensor of data to extract slices from. outputs: output : T Sliced data tensor. 总结来说，在opset10以前的版本，这些node导出来的时候，他们的属性值是不一样的，比如opset9，在attributes属性下，获取axes, starts, ends这些参数，而在opset10，直接通过inputs获取即可，其中还增加了steps参数，而这个还是可选的。 所以当你在打印一个onnx里面Slice层的时候是这样的： input: &quot;3020&quot;input: &quot;3029&quot;input: &quot;3030&quot;input: &quot;3028&quot;output: &quot;3031&quot;name: &quot;in: 3020;3029;3030;3028. out: 3031&quot;op_type: &quot;Slice&quot; 那么这个opset是10或者10以上，但假如你将这个onnx转到trt，得到了如下错误： onnx-tensorrt/builtin_op_importers.cpp:1803 In function importSlice:[8] Assertion failed: input_name == &quot;axes&quot; || input_name == &quot;steps&quot; 这是啥意思呢？错误出在这个地方： auto const&amp; input_name = node.input(3);ASSERT(input_name == &quot;axes&quot; || input_name == &quot;steps&quot;, ErrorCode::kUNSUPPORTED_NODE); 上面每一个input都是有名字的，我们打印这个input的名字来看一下？难道说，上面的input应该是 data, axes, steps? 难道不应该是上一个Node的名字吗？这个就有点纠结了. 有点跑题了，继续说我们的ONNX元运算。假如我们要保存一个很小的ONNX graph，小到只有两个Node，我们应该怎么做呢？其实也很简单： \"\"\"unit onnx node to tiny ones\"\"\"import onnxfrom onnx import helperfrom onnx import AttributeProto, TensorProto, GraphProtoimport numpy as npnode_conv = onnx.helper.make_node( 'Abs', inputs=['x'], outputs=['123'], name='122')node_def = onnx.helper.make_node( 'Slice', inputs=['x', '123', '123', '123', '123'], outputs=['y'], name='123')x = helper.make_tensor_value_info('x', TensorProto.FLOAT, [20, 10, 5])y = helper.make_tensor_value_info('y', TensorProto.FLOAT, [3, 10])W = helper.make_tensor_value_info('y', TensorProto.FLOAT, [3, 10])starts = helper.make_tensor_value_info( 'starts', TensorProto.INT64, [0, 0])ends = helper.make_tensor_value_info( 'ends', TensorProto.INT64, [3, 10])axes = helper.make_tensor_value_info( 'axes', TensorProto.INT64, [0, 1])steps = helper.make_tensor_value_info( 'steps', TensorProto.INT64, [1, 1])graph_def = helper.make_graph( [node_conv, node_def], 'testmodel', [x], [y],)model_def = helper.make_model(graph_def, producer_name='fucking company')onnx.checker.check_model(model_def)onnx.save(model_def, 'testmodel.onnx') 这里我们定义了一张图，里面其实只有两个node，一个是Abs，一个是Slice，但是实际上他们的运算没有任何意义。目的是让大家知道，如何保存这么一个元图。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"maskrcnn-benchmark转onnx再转TensorRT实录","slug":"2019_09_26_10_maskrcnn-benchmark转onnx再转TensorRT实录","date":"2019-09-26T02:29:37.000Z","updated":"2020-02-17T06:01:58.935Z","comments":true,"path":"2019/09/26/2019_09_26_10_maskrcnn-benchmark转onnx再转TensorRT实录/","link":"","permalink":"http://yoursite.com/2019/09/26/2019_09_26_10_maskrcnn-benchmark转onnx再转TensorRT实录/","excerpt":"本文介绍 maskrcnn-benchmark转onnx再转TensorRT实录","text":"本文介绍 maskrcnn-benchmark转onnx再转TensorRT实录 maskrcnn-benchmark转onnx再转TensorRT实录 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 这篇文章将记录一下，如何通过onnx将maskrcnn模型转到tensorrt。到目前位置，我们基本上可以转到onnx，并通过onnxruntime得到正确的推理结果。但唯一不好的地方在于，无法转到tensorrt，问题主要在于： onnx为dynamic input的尺寸，这在TensorRT里面是无法支持的； 当你将输入尺寸固定之后，输出尺寸也随之固定了，这种情况下模型基本上无法推理。 针对这么一种情况，解决方案不是没有，但！ 当我升级到TensorRT6.0和onnx-tensorrt对应的6.1的时候，惊奇的发现，dynamic的输入也是被支持的！！那么这个时候情况就变了哇！！我们接下来就从TensorRT6.0开始踩一踩坑！ 遇到的第一个错误是这个： While parsing node number 404 [Cast -&gt; &quot;407&quot;]:--- Begin node ---input: &quot;406&quot;output: &quot;407&quot;name: &quot;406&quot;op_type: &quot;Cast&quot;attribute &#123; name: &quot;to&quot; i: 1 type: INT&#125;--- End node ---ERROR: onnx-tensorrt/builtin_op_importers.cpp:317 In function importCast:[8] Assertion failed: trt_dtype == nvinfer1::DataType::kHALF &amp;&amp; cast_dtype == ::ONNX_NAMESPACE::TensorProto::FLOAT 这告诉我们，Cast这个node在进行类型h转换的时候出现了错误，再看看onnx-tensorrt的源码 builtin_op_importers.cpp： DEFINE_BUILTIN_OP_IMPORTER(Cast) &#123; // Get input node. OnnxAttrs attrs(node); auto cast_dtype = attrs.get&lt;int32_t&gt;(\"to\"); auto * tensor_ptr = &amp;convertToTensor(inputs.at(0), ctx); auto trt_dtype = tensor_ptr-&gt;getType(); // TensorRT currently only supports the following conversion: FP16 -&gt; FP32. ASSERT(trt_dtype == nvinfer1::DataType::kHALF &amp;&amp; cast_dtype == ::ONNX_NAMESPACE::TensorProto::FLOAT, ErrorCode::kUNSUPPORTED_NODE); // Add the layer. nvinfer1::IIdentityLayer* layer = ctx-&gt;network()-&gt;addIdentity(inputs.at(0).tensor()); layer-&gt;setPrecision(nvinfer1::DataType::kFLOAT); RETURN_FIRST_OUTPUT(layer);&#125; ​ 这行assertion就是错误的根源，从这个函数来看，它是把FP16转到了FP32, 错误信息并没有告诉我们这个node是将哪个类型转到哪个类型。这个时候咋办？改源码吧，打印一下，它是从哪个类型转到哪个类型。 其实不用改源码，查看一下maskrcnn.onnx里面的每一个Cast大概既知道了： node: 6622, op_type: Cast | input: [&apos;6659&apos;], output: [&apos;6660&apos;]input: &quot;6659&quot;output: &quot;6660&quot;name: &quot;6622&quot;op_type: &quot;Cast&quot;attribute &#123; name: &quot;to&quot; i: 7 type: INT&#125; 它是将类型转换到INT！但是很显然，这里只支持转到FLOAT32！也即是说，这里的所有的Cast节点似乎都是错误的。因为他们应该转到FLOAT32或者直接去掉不用！ 那就继续走这条路吧，解决一下datatype不对的问题。 Cast不支持转到INT64，怎么整？假设这个问题可以解决掉，接下来是第二个问题： While parsing node number 413 [Resize -&gt; &quot;416&quot;]:ERROR: /media/fagangjin/wd/permanent/software/source_codes/dl/onnx-tensorrt/builtin_op_importers.cpp:2270 In function importResize:[8] Assertion failed: scales.is_weights() 可以看到，这里的resize是这样的： node: 415, op_type: Resize | input: [&apos;388&apos;, &apos;415&apos;], output: [&apos;416&apos;]input: &quot;388&quot;input: &quot;415&quot;output: &quot;416&quot;name: &quot;415&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;node: 447, op_type: Resize | input: [&apos;420&apos;, &apos;447&apos;], output: [&apos;448&apos;]input: &quot;420&quot;input: &quot;447&quot;output: &quot;448&quot;name: &quot;447&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;node: 479, op_type: Resize | input: [&apos;452&apos;, &apos;479&apos;], output: [&apos;480&apos;]input: &quot;452&quot;input: &quot;479&quot;output: &quot;480&quot;name: &quot;479&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125; 可以看到，好像并没有scales 的 weights，resize没有scales参数，这个怎么整。 这是第二条线路，第二条线路是先简化模型，然后再convert。 但也遇到了本质上的问题，比如说，这个简化的原理，实际上是根据指定尺寸输入，中间层的一些Shape，Gather, Unsqueeze等都会成为Constant，这些Constant可以被优化掉，不信可以对比一下优化前后的两个图： 实际上，像Batchnorm这样的层被优化掉了，这个优化来自于官方的优化，optimize，其次类似于Shape、Gather、Unsqueeze这样的层，假如输入尺度确定之后，这些node的参数应该也是被确定的，因此可以直接用来存为Constant，当做常数来用。 但是这一套机制对于动态的输入并没有用。此时这条路应该是走不通的。 mmdetection导入onnx转TensorRT上面路子行不通，尝试采用mmdetection导出的onnx，转trt。我们可以比较轻松地把里面的Batchnorm等层去掉，同时对模型进行一下抛光，但是当转到trt的时候，就会遇到各种各样的问题： node: in: 3020;3029;3030;3028. out: 3031, op_type: Slice | input: [&apos;3020&apos;, &apos;3029&apos;, &apos;3030&apos;, &apos;3028&apos;], output: [&apos;3031&apos;]input: &quot;3020&quot;input: &quot;3029&quot;input: &quot;3030&quot;input: &quot;3028&quot;output: &quot;3031&quot;name: &quot;in: 3020;3029;3030;3028. out: 3031&quot;op_type: &quot;Slice&quot; 比如说Slicenode会出现错误，而且这个错误是来自于这个： ASSERT(input_name == &quot;axes&quot; || input_name == &quot;steps&quot;, ErrorCode::kUNSUPPORTED_NODE); 完全不知所云啊，我们的slice好像并没有这么一个所谓的名字，所以就出现了错误。这就奇怪了，是我们导出的Slice有问题，还是啥问题，为什么onnxruntime里面能够推理，到了onnx-trt里面就不行了呢？ 这样把，为了彻底搞明白这个问题，现在需要做一个实验，及调用一切可以用到的资源来进行onnx一个node的运算，这个我叫做元运算(unit onnx), 通过元运算，我们看看这么一个node到底是如何实现前向运算的。 为了解决上诉slice问题，我们修改onnx-trt的源码: 通过去掉这个assertion，把axes的assign加上，基本上可以解决这个问题了。那么第二个问题来了： onnx-tensorrt/builtin_op_importers.cpp:2274 In function importResize:[8] Assertion failed: scales.is_weights() Resize导致的问题，要解决这个问题，深入研究一下先。 首先看一下这个Resize在onnx里面是如何定义： Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor. Each dimension value of the output tensor is: output_dimension = floor(input_dimension (roi_end - roi_start) scale) if input “sizes” is not specified. 这个op，在opset10和opset11上是不一样的。输入输出不一样，这个就很蛋疼，说实话。 opset10里面的Resize： # Inputs X scales# Outputs Y# Attributes mode (nearest, linear, bilinear) opset11里面的Resize: # Inputs X roi scales sizes# Outputs Y# Attributes coordinate_transformation_mode cubic_coeff_a exclude_outside extrapolation_value mode nearest_mode 看到了吗？？？这个Resize在opset10和opset11上操作是不同的。当你把model里面的Resize打印出来之后： input: &quot;571&quot;input: &quot;598&quot;output: &quot;599&quot;name: &quot;in: 571;598. out: 599&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;[name: &quot;mode&quot;s: &quot;nearest&quot;type: STRING]571598 看到了吗？这里的input没有名字，但是第二个，应该scales，如果我没有猜错的话。那么看看onnx-tensorrt里面是怎么暴力处理这个node的： nvinfer1::ITensor&amp; input = convertToTensor(inputs.at(0), ctx); int input_dims = input.getDimensions().nbDims; ASSERT(input_dims &gt; 0, ErrorCode::kUNSUPPORTED_NODE); // Add resize layer nvinfer1::IResizeLayer* layer = ctx-&gt;network()-&gt;addResize(input); // Retrive and validate scale factors. // Scale factors include batch dimensions as well. ASSERT(inputs.size() == 2, ErrorCode::kINVALID_NODE); auto scales = inputs.at(1); // Support for scales as weights // std::cout &lt;&lt; \"uncomment scales.is_weights().\\n\"; ASSERT(scales.is_weights(), ErrorCode::kUNSUPPORTED_NODE); ShapedWeights scales_weights = scales.weights(); ASSERT(scales_weights.shape.nbDims == 1, ErrorCode::kUNSUPPORTED_NODE); ASSERT(scales_weights.count() == static_cast&lt;size_t&gt;(input_dims), ErrorCode::kUNSUPPORTED_NODE); ASSERT(scales_weights.type == ::ONNX_NAMESPACE::TensorProto::FLOAT, ErrorCode::kINVALID_NODE);......... 问题就出在这里了，这里的scales明明就是 inputs.at(1)第二个input没有错啊，但是判断 scales.is_weights()的时候不对了，那么我想知道，这个 is_weights到底是个啥子玩意？？ 花了一点时间，开发了一个onnx模型的explore工具：https://github.com/jinfagang/onnxexplorer 磨刀不误砍柴工。接下来需要借助这个工具来解决一下Resize这个问题。简单的来说，解决Resize layer这个is_weights 判断问题。 首先我们用onnxexp 来看一下这个模型里面都有啥： $ onnxexp bbb.onnx search -t &apos;Resize&apos; Exploring on onnx model: bbb.onnxsearch node by ID: Resizeinput: &quot;542&quot;input: &quot;569&quot;output: &quot;570&quot;name: &quot;in: 542;569. out: 570&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;input: &quot;571&quot;input: &quot;598&quot;output: &quot;599&quot;name: &quot;in: 571;598. out: 599&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;input: &quot;600&quot;input: &quot;627&quot;output: &quot;628&quot;name: &quot;in: 600;627. out: 628&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;listed all 3 Resize nodes in detail. 搜索了一下，Resize的op也就三个，似乎不是什么大问题啊，这里对比一下yolov3.onnx里面的Resize node看看： $ onnxexp yolov3.onnx search -t &apos;Resize&apos;Exploring on onnx model: yolov3.onnxsearch node by ID: Resizeinput: &quot;model_1/leaky_re_lu_59/LeakyRelu:0&quot;input: &quot;Resize_scales&quot;output: &quot;model_1/leaky_re_lu_59/LeakyRelu:0_permuted_upsampled&quot;name: &quot;Resize1&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;doc_string: &quot;&quot;domain: &quot;&quot;input: &quot;model_1/leaky_re_lu_66/LeakyRelu:0&quot;input: &quot;Resize_scales&quot;output: &quot;model_1/leaky_re_lu_66/LeakyRelu:0_permuted_upsampled&quot;name: &quot;Resize&quot;op_type: &quot;Resize&quot;attribute &#123; name: &quot;mode&quot; s: &quot;nearest&quot; type: STRING&#125;doc_string: &quot;&quot;domain: &quot;&quot;listed all 2 Resize nodes in detail. 可以看到，它这个Resize的input比较正常一些，Resize_scales是输入的scales，我们看看这个scales到底是个啥？主要看看是不是weights。 经过一番debug之后，发现这个Resize的op似乎有点不太正常啊，并没有这个scales的输入，那咋办呢。其实这个scales的来源是有的，来自于concat的输出。 总结最后总结一下，通过onnx转到maskrcnn存在以下问题： 一些支持的op比如Resize，并非真正的支持，Resize如果scales来自另外一个node的输出将不支持，只支持写死的scales参数； 一些层不支持如RoIAlign，这个层需要写Plugin去支持， 另外一些不支持的包括： NonMaxSuppression, Equal, ReverseSequence(mmdet version only), NonZero，Scatter（fb version only)，； 解决办法和难点： 一些不支持的op如ROIAlign可以通过在onnx-tensorrt里面自己写importer来支持导入，然后根据inputs和outputs来编写逻辑，但对于一些已经被支持的层但无法处理特殊情况的op，无法debug到底正确操作应该是啥样的，比如onnx-tensorrt官方里面将Resize parse的时候第二个inputs parse为weights，这通常情况下是initializers，但模型里面来自于上一个node的输出，意味着这个层无法再转engine的过程中被构建。Slice层onnx-tensorrt在处理的时候会将一些位置参数按照标准格式来处理，但对于一些trace的模型一些参数的名称并非严格按照标准来的，通过修改onnx-tensorrt源码可以给它修改。 至少需要重写Resize层，增加ROIAlign，NMS，Nonzeros，Equal等层，工作量还是比较大。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"FOTS和ROIRotate以及仿射变换","slug":"2019_09_12_11_FOTS和ROIRotate以及仿射变换","date":"2019-09-12T03:16:25.000Z","updated":"2020-02-17T06:07:29.843Z","comments":true,"path":"2019/09/12/2019_09_12_11_FOTS和ROIRotate以及仿射变换/","link":"","permalink":"http://yoursite.com/2019/09/12/2019_09_12_11_FOTS和ROIRotate以及仿射变换/","excerpt":"本文介绍 FOTS和ROIRotate以及仿射变换","text":"本文介绍 FOTS和ROIRotate以及仿射变换 FOTS和ROIRotate以及仿射变换 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu FOTS是一个速度比较快，同时是一个端到端的文本检测方法，这个方法有一些东西值得学习，这也是写这篇文章的原因。通常我们实现一个文本检测器应该需要定位和识别两步，比如用CTPN定位，用CRNN识别，但是他们的速度都太慢了，二FOTS几乎做到实时。 除了端到端这个优点之外，FOTS还提出了一个很好的思想，即是ROIRotate，这对于如何采用ROI来回归带旋转角度的框提供了一些思考和价值。在开始之前，需要向大家介绍一下仿射变换。 仿射变换(Affine Transform)其实这个大家并不陌生，但是要从原理上理解不容易。说白了，仿射变换就是一种线到线或者平面到平面的不改变其平面特性的变换，比如你是平行的线，仿射变换之后还是平行的。 我们尝试将一张图片来进行仿射变换，因为它本身是一个矩形，我们可以看看这个矩形像素区域经过仿射变换之后会变成什么？ 这个仿射变换可以用下面的代码来轻松得到： import cv2import numpy as npimport sysimg = cv2.imread(sys.argv[1])M = np.array( [[1, 0, 100], [0, 1, 50]], dtype=np.float32)res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))cv2.imshow('affine', res)cv2.imshow('original', img)cv2.waitKey(0) 请注意这里的核心是我们的矩阵M。这个矩阵描述的就是仿射变换的方式。 上面这个矩阵的意思是，x移动100，y移动50，实际上就是一个最简单的仿射变换形式。那我们再来复杂一点的，我们将来开始缩放了。 实际上opencv里面有一个函数很有用，比如我们要在坐标(30, 30)的位置，顺时针旋转45度，我如何才能计算出这个需要的旋转矩阵呢？简单： M = cv2.getRotationMatrix2D((30, 30), 45, 1) 这个函数最后一个代表是缩放，我们尝试一下： import cv2import numpy as npimport sysimg = cv2.imread(sys.argv[1])M = cv2.getRotationMatrix2D((30, 30), 45, 0.7)res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))cv2.imshow('affine', res)cv2.imshow('original', img)cv2.waitKey(0) 那很显然，我们在（30， 30）这个位置进行了旋转，但好像默认方向是逆时针。同时对图片进行了0.7倍的缩放。 好了，想必大家对于仿射变换的理解已经超越我了，那我们再来难度加大一点的，我们将进行错切。等等，你说的平移、缩放、旋转都能理解，错切是什么东西？ 并不陌生，那种带有很多平行四边形的晾衣架见过么？你把它扭动一下这个操作就叫做错切。但是错切有什么用呢？错切最大的用途应该就是透视了，还记得小时候画画学画屋顶吗？怎么才能让屋顶具有3D感？那不就是利用透视和错切吗？ 那用到文本检测又有啥用呢？现实中很多文字都是要通过透视才能精准的扣出它的位置的，此时错切就用得上了。好废话说了那么多，错切长啥样？ 实际上opencv里面也提供了这么一个函数来拿到我们需要错切变换矩阵，在opencv了里面就叫做affineTransform： M=cv2.getAffineTransform(pts1, pts2) 这里我们给出两个位置，每个位置三个点，就可以唯一确定一个仿射变换，这里必须要明白一个原理，那就是仿射变换在两对三个点之间是唯一确定的。 import cv2import numpy as npimport sysimg = cv2.imread(sys.argv[1])pts1 = np.float32([[50, 50], [200, 50], [50, 200]])pts2 = np.float32([[10, 100], [200, 150], [100, 250]])M = cv2.getAffineTransform(pts2, pts1)res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))cv2.imshow('affine', res)cv2.imshow('original', img)cv2.waitKey(0) 实际上这就是仿射变换。同样的道理，我们通过一个错切的图片，可以通过矫正，将其转换到正常的图片，比如我们从一张图片抠出来了照片拍出来的放映的PPT，此时PPT是错切的，通过放射变换，可以将这个区域矫正为正的格式。 通过opencv里面的透视变换可以反过来将不规则的四边形纠正为正的四边形。此时需要四个点进行纠正，我们在这里暂且不谈，感兴趣的便宜可以做一些尝试，示例代码： img = cv2.imread('card.jpg')# 原图中卡片的四个角点pts1 = np.float32([[148, 80], [437, 114], [94, 247], [423, 288]])# 变换后分别在左上、右上、左下、右下四个点pts2 = np.float32([[0, 0], [320, 0], [0, 178], [320, 178]])# 生成透视变换矩阵M = cv2.getPerspectiveTransform(pts1, pts2)# 进行透视变换，参数3是目标图像大小dst = cv2.warpPerspective(img, M, (320, 178)) 话说回来，那么接下来需要做什么事情呢？当然啦，今日的主角是FOTS。 FOTS快速的文本检测方法","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"onnx-tensorrt实现添加自己的模型plugin","slug":"2019_09_11_18_onnx-tensorrt实现添加自己的模型plugin","date":"2019-09-11T10:17:56.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/09/11/2019_09_11_18_onnx-tensorrt实现添加自己的模型plugin/","link":"","permalink":"http://yoursite.com/2019/09/11/2019_09_11_18_onnx-tensorrt实现添加自己的模型plugin/","excerpt":"本文介绍 onnx-tensorrt实现添加自己的模型plugin","text":"本文介绍 onnx-tensorrt实现添加自己的模型plugin onnx-tensorrt实现添加自己的模型plugin This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 总结一下现在基本上深度学习模型部署的套路： 一般都是把模型training然后export 到onnx； 但是onnx如何推理呢，要么使用一些backend，或者使用onnx-tensorrt; 实际上使用tensorrt加速基本上是大势所驱啊。tensorrt目前支持的层不少，要是能再多一些，那么基本上大部分的层都可以采用tensorrt来推理了。但是有时候呢，有一些层是不支持的，此时要想尽一切办法来进行补充。举个例子，比如maskrcn模型，截止到目前，它的模型支持情况可以如下所示： All ops used in MaskRCNN-FPN-Resnet50:MaxPoolConstantOfShapeConstantOfShape not supported in onnx_tensorrt or TensorRT 5.1 now.TopKExpGatherNonZeroNonZero not supported in onnx_tensorrt or TensorRT 5.1 now.ReshapeReduceMinGreaterGreater not supported in onnx_tensorrt or TensorRT 5.1 now.TransposeSqrtShapeAddSplitSqueezeDivGemmNonMaxSuppressionConstantConvExpandExpand not supported in onnx_tensorrt or TensorRT 5.1 now.NotNot not supported in onnx_tensorrt or TensorRT 5.1 now.SliceEqualEqual not supported in onnx_tensorrt or TensorRT 5.1 now.CastLogAndAnd not supported in onnx_tensorrt or TensorRT 5.1 now.SubResizeRoiAlignScatterScatter not supported in onnx_tensorrt or TensorRT 5.1 now.MulClipUnsqueezeLessLess not supported in onnx_tensorrt or TensorRT 5.1 now.ConcatSoftmaxFloorReluSigmoidConvTransposeFlatten 可以看到，只有一些看起来十分简单的层不支持，其他的，其实都支持。但是这些层看似简单，实现起来呢？我想应该不简单。比如说这么一些比较层，逻辑层等等。 我们将尝试在onnxtrt中加入Bi-linear Upsamling 层，也就是双线性差值上采样层，来探索一下在onnxtrt中添加一个层的难度。 ## 寻找一个测试模型 要添加BilinearUpsampling层就得先知道这个层可以用哪个模型来测试，这个模型要满足两个条件： 直接用onnx2trt无法转换的，因为它包含BilinearUpsampling层； 其他的层都应该被支持。 这样通过我们的添加之后，我们的onnx-tensorrt就可以转换它，从而进一步的尝试使用TensorRT的推理引擎来load这个engine进行推理。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"LightHeadRCNN实现细节思考与Debug","slug":"2019_07_22_14_LightHeadRCNN实现细节思考与Debug","date":"2019-07-22T06:25:23.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/22/2019_07_22_14_LightHeadRCNN实现细节思考与Debug/","link":"","permalink":"http://yoursite.com/2019/07/22/2019_07_22_14_LightHeadRCNN实现细节思考与Debug/","excerpt":"本文介绍 LightHeadRCNN实现细节思考与Debug","text":"本文介绍 LightHeadRCNN实现细节思考与Debug LightHeadRCNN实现细节思考与Debug This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu lightheadRCNN当中存在一个Proposal模块，这个与SSD是不同的。最近在训练LightHeadRCNN过程中频繁出现一些小问题。这里做一个记录。 ## Proposal模块 proposal这里的nms，注意有很多次。所谓的提建议模块，说白了，就是给网络提出候选框的。这里候选框一般称作为ROIs。ROI和anchors有啥区别呢？其实就是一个坐标转换的关系。但凡是anchors都是坐标归一化的东西，从anchors转换到ROI，那么就需要进行decode，解码坐标，从而将坐标转换到图片尺寸下。 其实proposal实际上做的工作就是，将RPN网络输出的anchors（筛选之后的anchor），进行nms之类的工作，是得它更加的纯净。那Proposal之后出来的ROI实际上就已经差不多是比较准确的物体位置了（但仅仅限于anchor所能产生的位置，对于anchor无法覆盖的区域是不准确的，需要微调）。 发现的一些问题： 使用Resnet101作为后端，如何网络从0开始训练，在最开始的时候roi很多宽度为0，这样的话你很可能给到loss计算的roi就没有了（被min_size给filter掉了）此时应该怎么办？当然用与训练权重是可以的。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"tensorrt推理onnx模型（二）","slug":"2019_07_18_14_tensorrt推理onnx模型（二）","date":"2019-07-18T06:00:49.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/18/2019_07_18_14_tensorrt推理onnx模型（二）/","link":"","permalink":"http://yoursite.com/2019/07/18/2019_07_18_14_tensorrt推理onnx模型（二）/","excerpt":"本文介绍 tensorrt推理onnx模型（二）","text":"本文介绍 tensorrt推理onnx模型（二） tensorrt推理onnx模型（二） This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 继续上一篇的探索，实际上由于上一篇的api已经过时，最新的TensorRT5.1的API已经完全变了。 我们依旧以mobilenet为例，实际上我们实现将mobilenet转到trt，也就是直接序列化为一个engine，然后直接load这个engine就可以推理了。其实也很简单直接： import tensorrt as trtimport pycuda.driver as cudaimport pycuda.autoinitimport cv2import sysimport osimport numpy as npTRT_LOGGER = trt.Logger(trt.Logger.WARNING)def build_engine(engine_f): with open(engine_f, 'rb') as f, trt.Runtime(TRT_LOGGER) as runtime: return runtime.deserialize_cuda_engine(f.read()) def init(engine_f): engine = build_engine(engine_f) print(engine.get_binding_shape(0)) print(engine.get_binding_shape(1)) # 1. Allocate some host and device buffers for inputs and outputs: h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(trt.float32)) h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(trt.float32)) # Allocate device memory for inputs and outputs. d_input = cuda.mem_alloc(h_input.nbytes) d_output = cuda.mem_alloc(h_output.nbytes) # Create a stream in which to copy inputs/outputs and run inference. stream = cuda.Stream() context = engine.create_execution_context() return context, h_input, h_output, stream, d_input, d_outputdef load_input_data(img_f, pagelocked_buffer, target_size=(224, 224)): upsample_size = [int(target_size[1] / 8 * 4.0), int(target_size[0] / 8 * 4.0)] img = cv2.imread(img_f) img = cv2.resize(img, target_size, interpolation=cv2.INTER_CUBIC) # Flatten the image into a 1D array, normalize, and copy to pagelocked memory. np.copyto(pagelocked_buffer, img.ravel()) return upsample_sizedef predict2(): global context, h_input, h_output, stream, d_input, d_output upsample_size = load_input_data(sys.argv[1], h_input, target_size=(224, 224)) cuda.memcpy_htod_async(d_input, h_input, stream) context.execute_async(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle) cuda.memcpy_dtoh_async(h_output, d_output, stream) stream.synchronize() print(np.argmax(h_output))if __name__ == \"__main__\": context, h_input, h_output, stream, d_input, d_output = init('mobilenetv2-1.0/mobilenetv2-1.0.trt') predict2() 上面这个代码是完整的推理代码，其中很多模块可以复用。 这个只是一次简单的流程，由于速度上并没有优势，因此直接忽略时间上的计算。总结来说，通过tensorrt加速推理分为这么几个步骤： 首先要建造引擎，两种方式，先将onnx转到trt，直接load trt反序列化即可得到引擎，另一种是用onnxparser来得到engine，第二种本质上效率更低，因为有一个步骤可以事先省略； 根据engine的输入输出信息，来开辟cuda内存，其中cuda上的内存叫做d，宿主机的叫做h； 拿到了引擎返回的context，就可以执行了，执行也很简单，把binding传入即可。 这个tensorrt差不多就是这么个意思，但还有一些细化的东西需要理解，比如： 我如果一个输入，三个输出怎么办？ 网络输出的结果如何进行后处理？ C++要怎么做？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"onnx模型tensorrt推理实践","slug":"2019_07_16_20_onnx模型tensorrt推理实践","date":"2019-07-16T12:59:26.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/16/2019_07_16_20_onnx模型tensorrt推理实践/","link":"","permalink":"http://yoursite.com/2019/07/16/2019_07_16_20_onnx模型tensorrt推理实践/","excerpt":"本文介绍 onnx模型tensorrt推理实践","text":"本文介绍 onnx模型tensorrt推理实践 onnx模型tensorrt推理实践 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 现在很多场景模型部署都是用tensorrt，因为显卡速度比cpu快很多，加上一些场景还真的不在乎成本，因此为了使得系统更加稳定可靠，用tensorrt就显得很不错了，典型的场景比如自动驾驶。 那么我觉得最重要的是，如何了解tensorrt，以及如果将onnx模型用tensorrt进行推理。 tensorrt 安装tensorrt的安装就比较简单了，现在常用的是tensorrt5.1. 你可以下载cuda对应的版本，下载之后，主要进行这么几个步骤： 添加lib 到path里面： export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/home/xx/tensorrt5.1/lib:$LD_LIBRARY_PATH 假设你的tensorrt解压在home目录。这样也是为了后续方便。 编译samples tensorrt自带了几个sample，可以cd进去make，然后在上一层的bin里面可以看到可执行文件。 安装onnx-tensorrt 打通onnx和tensorrt的桥梁 直接clone然后编译： git clone https://github.com/onnx/onnx-tensorrt.git --recursivecmake .. -DTENSORRT_ROOT=~/tensorrt5.1 然后就可以得到一个可执行的二进制文件：onnx2trt. 这样的话就可以将onnx转到trt上。 那么问题来了，我为什么要安装在home目录呢？等一下include的时候cmake又无法找到文件。。。那就直接在cmake里面指定吧，总感觉直接丢到系统目录不是一件好事。 onnx模型转trt那么问题来了，如何将onnx转到tensorrt呢？onnx有一个onnx_tensorrt的转换工具，编译之后即可转换。比如我们要将mobilenetv2的onnx模型转到trt，那么： onnx2trt mobilenetv2-1.0.onnx -o mobilenetv2-1.0.trt 需要注意的是，上面的trt实际上就是engine了已经。但是问题是这个trt，怎么欧通过tensorrt来做inference。 使用tensorrt推理trt模型既然转到trt模型，那么用tensorrt如何推理呢。实际上tensorrt可以直接推理onnx模型，但是本质上还是要在代码中将模型转换为trt再进行推理的。 假设我们刚才通过onnx2trt工具，已经得到了一个engine模型，也就是说mobilenetv2_engine.trt. 我们用python代码来尝试inference一下： import tensorrt as trtG_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogServerity.INFO)engine = trt.utils.load_engine(G_LOGGER, 'mobilenetv2_engine.trt')# create an execution contextcontext = engine.create_execution_context()# allocate inputs and outputsimport pycuda.driver as cudaimport pycuda.autoinitd_input = cuda.mem_alloc(input_shapes)d_output = cuda.mem_alloc(output_shapes)bindings = [int(d_input), int(d_output)]# feed input data and execute inferencestream = cuda.Stream()cuda.memcpy_htod_async(d_input, data, stream)context.enqueue(batch_size, bindings, stream.handle, None)cuda.memcpy_dtoh_async(output, d_output, stream)stream.synchronize()# or do something like thiscuda.memcpy_htod(d_input, data)context.execute(batch_size, bindings)cuda.memcpy_dtoh(output, d_output) 此时，我们应该已经将trt的engine 加载进来了。 python的例子是有了，但是C++的没有。 上面的api是老的tensorrt的api，新版的已经有很大的不一样了。具体看下一篇报道。 参考文献或网址 https://faldict.github.io/faldict/tensorrt-summary/","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"下一站-最舍不得的一次离职","slug":"2019_07_12_11_下一站-最舍不得的一次离职","date":"2019-07-12T03:53:45.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/12/2019_07_12_11_下一站-最舍不得的一次离职/","link":"","permalink":"http://yoursite.com/2019/07/12/2019_07_12_11_下一站-最舍不得的一次离职/","excerpt":"本文介绍 下一站-最舍不得的一次离职","text":"本文介绍 下一站-最舍不得的一次离职 下一站-最舍不得的一次离职 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 准备离职，职场的下一站到站了。 最后一天花了点时间，记录一下此时此刻的感悟。搞了很长时间的技术，一直没有时间提笔写一些东西，今天正好，万籁寂静，微风拂面，且提笔记录此刻心境。 职场如三国我一直将职场比作是三国，英雄辈出，商场就如同战场一般，上一刻你输了，下一刻你可能就笑到了最后。 世界千万事莫不如此。此次卸任最让我舍不得是还是人情，很多人都在抱怨996，中国员工被压榨，但我觉得不然，公司与员工应当是一个命运的共同体，只是平时你很难察觉到。 与HR和领导交谈完，我明白他们舍不得，倒不是说我有多能干，而是在乱世之中，复杂变化莫测的商场之中，缺了一个可信赖的战友，会觉得腹背受敌。 我不知道何时我才会有这种感觉，但毫无疑问，人才，就像是当年三国一样，诸葛亮可以收留曹魏的判降之人姜维，而日后他接替诸葛亮成蜀国的抵住。 ## 新的起点 我不知道等待我的是什么样的命运，但我想将自己的阅历放到人生的大尺度上来看，也许这样一次职业变动就像是大海的一朵浪花，及其的平凡和不起眼。当然，也许，一些细微的变动可能像蝴蝶的翅膀，轻轻煽动便能引发飓风。时好时坏，看天，也看人。 是否准备妥当只要人在，青山便在。兄弟保重，各奔前程！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"pytorch module权重共享的诡异操作","slug":"2019_07_10_17_pytorch_module权重共享的诡异操作","date":"2019-07-10T09:34:11.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/10/2019_07_10_17_pytorch_module权重共享的诡异操作/","link":"","permalink":"http://yoursite.com/2019/07/10/2019_07_10_17_pytorch_module权重共享的诡异操作/","excerpt":"本文介绍 pytorch module权重共享的诡异操作","text":"本文介绍 pytorch module权重共享的诡异操作 pytorch module权重共享的诡异操作 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 记录一个十分诡异的操作。 事情的经过是这样的，我在对一个模型进行trace的过程中遇到一个错误： File &quot;/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py&quot;, line 1469, in __init__ check_unique(param) File &quot;/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py&quot;, line 1461, in check_unique raise ValueError(&quot;TracedModules don&apos;t support parameter sharing between modules&quot;)ValueError: TracedModules don&apos;t support parameter sharing between modules 这个错误的大概意思是：trace module的时候，不允许权重共享。说白了，trace就是生成一个字典，每个键就是module的attribute，值就是权重。pytorch在trace的时候发现，同一个module权重一个但是attribute却有两个，这种情况等于是两个键对应同一个值。讲道理这不是个问题，但是pytorch在trace模型的时候就是不支持。 不过这抛给我们一个问题：pytorch里面什么才算是权重共享？ 如何权重共享？至于我们是如何实现权重共享的，我们可能会经常有这样的操作： class BasicConv(nn.Module): def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False): super(BasicConv, self).__init__() self.out_channels = out_planes self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias) self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None self.relu = nn.ReLU(inplace=True) if relu else None def forward(self, x): x = self.conv(x) if self.bn is not None: x = self.bn(x) if self.relu is not None: x = self.relu(x) return x 因此我们有很多Conv+BN+Relu的堆叠块啊，为什么不把他们定义一个模块呢？然后接着我们就会这样用: class FuckNet(nn.Module): def __init__(): self.block_1 = BasicConv(256, 512, 3, 0, 1) self.fuck_layers = nn.Sequential() for i in range(5): self.fuck_layers.add_module('&#123;&#125;'.format(i), BasicConv(256, 256, 3, 0, 1)) 然后你用这个去训练一个模型，trace的时候就会出现上述的错误。 那么问题来了。上面的网络我如果这样写： class FuckNet(nn.Module): def __init__(): self.block_1 = BasicConv(256, 512, 3, 0, 1) self.fuck_layers = nn.Sequential() for i in range(5): self.fuck_layers.add_module('&#123;&#125;'.format(i), self.block_1) 二者是一样的吗？答案是不一样！！！二者不对等！！！ 总结如下： 首先这两个写法肯定是不一样的。 对于后一种写法，权重是肯定共享的。因为你5个层都是调用的它； 第一种写法，到底是共享还是不共享呢？？？？ 这个问题必须得有一个结论。 通过一个实验来测试一下： # -----------------------## Copyright Jin Fagang @2018# # 7/10/19# trace_test# -----------------------from torch import nnfrom alfred.utils.log import logger as loggingfrom alfred.dl.torch.common import deviceimport torch\"\"\"we want test which one can be trace\"\"\"class BasicConv(nn.Module): def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False): super(BasicConv, self).__init__() self.out_channels = out_planes self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias) self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None self.relu = nn.ReLU(inplace=True) if relu else None def forward(self, x): x = self.conv(x) if self.bn is not None: x = self.bn(x) if self.relu is not None: x = self.relu(x) return xclass FuckNet(nn.Module): def __init__(self): super(FuckNet, self).__init__() self.welcome_layer = BasicConv(3, 256, 3, 1, 1) self.fuck_layers = nn.Sequential() for i in range(5): self.fuck_layers.add_module('&#123;&#125;'.format(i), BasicConv(256, 256, 3, 1, 1)) def forward(self, x): x = self.welcome_layer(x) return self.fuck_layers(x)class FuckNet2(nn.Module): def __init__(self): super(FuckNet2, self).__init__() self.welcome_layer = BasicConv(3, 256, 3, 1, 1) self.block_1 = BasicConv(256, 256, 3, 1, 1) self.fuck_layers = nn.Sequential() for i in range(5): self.fuck_layers.add_module('&#123;&#125;'.format(i), self.block_1) def forward(self, x): x = self.welcome_layer(x) return self.fuck_layers(x)if __name__ == '__main__': model1 = FuckNet() model2 = FuckNet2() model2.eval().to(device) # start to trace model example = torch.rand(1, 3, 512, 512).to(device) traced_script_module = torch.jit.trace(model2, example) traced_script_module.save('test.pt') 这个简单的实验，先说一下结论： FuckNet2，这个无法trace； FuckNet，这个可以trace。 那么问题来了，为什么第二个网络不行？我的理解是这样的： 在给你的module添加子modul的时候，假如这个子module要用很多次，比如出现在一个for循环里面，那么这些个子的module就不能共享权重。原因就是一旦共享，意味着同一个权重有多个name或者id，生成tracedmodule的时候也就无法给这个权重分配name了，冲突了，因为我是按照权重来index的 要解决这个问题还真的没有好的方法。唯一的办法就是不让它共享。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"rust编写一个vim","slug":"2019_07_01_22_rust编写一个vim","date":"2019-07-01T14:01:30.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/07/01/2019_07_01_22_rust编写一个vim/","link":"","permalink":"http://yoursite.com/2019/07/01/2019_07_01_22_rust编写一个vim/","excerpt":"本文介绍 rust编写一个vim","text":"本文介绍 rust编写一个vim rust编写一个vim This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 关于rust的安装以及国内换源的一些铺垫操作，参考manaai社区的帖子. rust能干啥？能打败C++？能不能打败C++我们不讨论，我只想告诉大家，rust一门未来你一定要学会的语言。至于理由，我只给出两点： 够快，堪比C而不是C++； 优雅。 这个优雅，怎么说，来自于两方面，一方面是它从关键字到函数名甚至编程风格都很精简，比如定义函数就一个fn, 简单吧？另外一个方便，没有C++恶心的头文件和动态链接库版本混乱。rust有从零开始构建的包管理工具，每一个项目都有。如果用一句话来形容rust，我想这么说： 融合了Python的极客和C的快速的终极语言。 那么今天，我们从零实现一个vim。天下苦vim久已，但是大家应该都还没有牛逼到写一个vim的地步，但是相信我，看完本文，你至少可以实现一个你自己的nano。 高屋建甄当然我们没有必要从零去实现它。我们可以在已有的但是至今没有维护的版本上去构建，延伸。我采用的项目是itoa 这个项目到目前为止已经没有维护了。用cargo编译出错： Compiling iota-editor v0.1.0 (/Volumes/xs/tools/transfer/rust/iota)error[E0554]: #![feature] may not be used on the stable release channel --&gt; src/iota/lib.rs:9:1 |9 | #![feature(fn_traits)] | ^^^^^^^^^^^^^^^^^^^^^^error: aborting due to previous errorFor more information about this error, try `rustc --explain E0554`.error: Could not compile `iota-editor`.To learn more, run the command again with --verbose. 据说这是因为rust compiler太新导致了。运行这个： rustup override set nightly 这啥意思？ 编译在这个库卡了很久： Compiling termbox-sys v0.2.10 Building [================================================&gt; ] 31/36: ter... 我们看看termbox-sys 这个库是干啥的？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"深度学习量化之GEMM通用矩阵乘法","slug":"2019_06_27_14_深度学习量化之GEMM通用矩阵乘法","date":"2019-06-27T06:33:19.000Z","updated":"2020-02-17T06:09:33.745Z","comments":true,"path":"2019/06/27/2019_06_27_14_深度学习量化之GEMM通用矩阵乘法/","link":"","permalink":"http://yoursite.com/2019/06/27/2019_06_27_14_深度学习量化之GEMM通用矩阵乘法/","excerpt":"本文介绍 深度学习量化之GEMM通用矩阵乘法","text":"本文介绍 深度学习量化之GEMM通用矩阵乘法 深度学习量化之GEMM通用矩阵乘法 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu GEMM深度学习走到最后，就得从理论到应用部署。除了一些网络设计，新的优化方法，新的任务，已有任务的指标刷新等领域以外，深度学习模型的量化是一个很重要的领域，这基本上是深度学习的根基了，就好像建造上层操作系统的底层优化库一般，没有它，别人的系统可以做的很优化有顺畅而你不行。 那么GEMM是什么？一种通用矩阵乘法。我们来看一看卷积的运算方式。 通过一个卷积核，对原图进行运算，每一个卷积核将得到一个固定的输出，请注意，不管原图的通道是多少，经过一个卷积核运算之后，都会变成一个维度的。最终featuremaps的channel取决于你用多少个卷积。 实际上这种算法在执行效率上会很低。即便是最老的caffe，人家用的也是im2col算法，啥是im2col？你卷积的时候，可以每个卷积区域拉伸为一个Patch，如下面所示： 如上所示，现在相当于是将每个待卷积区域拉伸为一个Patch。这样就可以变成矩阵的相乘： 基本上GEMM就是一个网络的核心，因为大多数的运算都集中在卷积和全连接，而这两者的本质就是矩阵的惩罚，上面也可以看到卷积通过转换可以直接变为矩阵的乘法。 我觉得从零实现一下GEMM也就是卷积算法，是有必要的. 我们用伪代码来写一下两个矩阵 A:mxk 和 B: kxn 的矩阵乘法： for (int m=0; m &lt; M; m++) &#123;for (int n=0; n &lt; N; n++) &#123;C[m][n] = 0;for (int k=0; k , K; k++) &#123;C[m][n] += A[m][k] * B[k][n];&#125;&#125;&#125; 这个简单易懂，即便用C数组去写，差不多也是这个写法。 这个算法不考虑任何优化的话，其实还有许多值得深入思考的问题，比如： 同样是矩阵相乘，那么如果是不同的尺寸，效率有啥区别？比如，我们可以先遍历m，也可以先遍历n，如果m比n大应该用哪种顺序更好？ Winograd算法winograd说白了，就是用加法来代替矩阵的乘法。那么如何用加法来代替乘法呢？ 这图说明了，使用winnograd加速可以做到比CUDNN还快，实际上CUDNN中的实现是用GEMM实现的。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"记录今天遇到的两个坑","slug":"2019_06_25_18_记录今天遇到的两个坑","date":"2019-06-25T10:07:41.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/06/25/2019_06_25_18_记录今天遇到的两个坑/","link":"","permalink":"http://yoursite.com/2019/06/25/2019_06_25_18_记录今天遇到的两个坑/","excerpt":"本文介绍 记录今天遇到的两个坑","text":"本文介绍 记录今天遇到的两个坑 记录今天遇到的两个坑 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 今天遭遇了两个坑，都是关于C++的。总结来说就几点： 以后部署环境尽量，千万别，也没必要，不用，不需要自己从源码安装某个库。因为你无法保证别人编译安装的链接版本号就是一致的。相反，从apt统一安装是最好的方式； boost从apt安装是没问题的，你caffe编译应该是没问题的； linux这种的源码依赖方式说实话有时候问题很大，有时间得好好了解一下下一代操作系统以及操作系统编程语言，也就rust和fuchsia； 还有一个坑来自于ROS。 这个坑更大，源自于ros和libtorch的集成。事情是这样的。 我本想集成一个基于libtorch的程序进入ros，但是二者冲突，当我链接libtorch的时候，ros无法正确连接到对应的…. 库。当我把libtoch的链接库comment掉，一切正常。 可能的原因是我ros相关依赖真的没有，因为我卸载过一次boost，而这个卸载会把ros相关库一并卸载掉。那我tm就纳闷了，都TM卸载了，你怎么还能编译？？？？ 另外我在这种情况下，尽然可以编译出可执行程序，可我发现我tm连rosrun都没有，catkinmake你怎么做到的？？？ catkin_package( # INCLUDE_DIRS include # LIBRARIES semantic_seg # CATKIN_DEPENDS other_catkin_pkg # DEPENDS system_lib) 这行代码没有什么用，但是在ros里面，如果 你不加它，那rosrun就无法找到执行脚本！！！！ 无法找到！！太他妈的神奇的ros了 最后我发现一个很严重的问题： C++各种依赖很头疼，但是习惯也还好； 但是到了ROS，tm的起码复杂了1000倍！我认为替代C++应该先把ros这一套先干掉，太复杂了。 最后还有一个奇葩问题，我tm的窗口怎么只有关闭按钮了？？？ 真的是。。。。。。 最终发现可能原因 https://discuss.pytorch.org/t/issues-linking-with-libtorch-c-11-abi/29510/2 来自pytorch论坛的论述： So, it seems that I’ve found the issue. The TorchConfig.cmake has this piece of config:if (&quot;$&#123;CMAKE_CXX_COMPILER_ID&#125;&quot; STREQUAL &quot;GNU&quot;) set(TORCH_CXX_FLAGS &quot;-D_GLIBCXX_USE_CXX11_ABI=0&quot;)endif()Which forces GCC to use the old C++11 ABI. Is there any reason why libtorch is being built using the old ABI ? Because what happens is that every single dependency that I have on my project such as leveldb or grpc for instance, won’t be able to be linked because they were compiled with the newer C++11 ABI. And I guess that this is the most common situation for other projects.If I manually change GLIBCXX_USE_CXX11_ABI=1 and recompile the entire libtorch by myself, it works just fine, but that requires a lot of manual steps that aren’t quite easy to integrate into my CMake file. Not to mention that it’s really hard to figure out how to compile the libtorch correctly.Are there any plans to move to the newer C++11 ABI and thus allow other dependencies to be linked without problems ? Otherwise, people will have a lot of trouble to depend on the libtorch libraries. 这句话什么意思呢？也就说libtorch默认使用了比较老的版本的ABI，而我们的ros或者其他很多程序都用的新的ABI。他妈的这就导致了这个玩意跟其他任何库都可能无法一起编译！！ 这个人解释了为什么pytorch要用老的ABI编译： 这个是啥意思呢？他们用的是gcc4.9，而这个只能使用老的ABI，他们的理由就是低版本的高版本也能用，但是高版本的低版本是用不了的。 沃日，真tm的坑。这群facebook的程序员。 坑在继续. 感觉tm的ros跟libtorch是冤家对头啊！！ 你libtorch用低版本的编译的对吧？那么我在cmake里面去掉高版本的ABI可以吧？兼容你的低版本： add_compile_options(-std=c++11 -D_GLIBCXX_USE_CXX11_ABI=0) 可是问题是！！！！这个加入之后，他妈的，ros就无法执行了。ros不支持降低ABI。。。。。。。 我也是无语了。 最后是从源码编译pytorch。不过请注意，从源码编译的时候千万不要直接从master分支，会踩坑。 切换到v1.1.0分支，可以从源码编译，完了之后把我们需要的链接库替换掉即可。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"ROIPooling与ROIAlign的思考","slug":"2019_06_20_11_ROIPooling与ROIAlign的思考","date":"2019-06-20T03:18:39.000Z","updated":"2020-02-17T06:10:03.958Z","comments":true,"path":"2019/06/20/2019_06_20_11_ROIPooling与ROIAlign的思考/","link":"","permalink":"http://yoursite.com/2019/06/20/2019_06_20_11_ROIPooling与ROIAlign的思考/","excerpt":"本文介绍 ROIPooling与ROIAlign的思考","text":"本文介绍 ROIPooling与ROIAlign的思考 ROIPooling与ROIAlign的思考 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu ROIPooling 是FasterRCNN里面使用的层， 它的作用是抽取出合适的特征来做物体检测和位置回归，而ROIAlign是MaskRCNN中使用的，它的作用是抽取合适的特征同时解决ROIPooling中由于Pooling操作导致的特征对齐问题。 从上图可以看出，ROIPoolin经过两次舍弃操作之后，最后的特征图，跟原始图片的位置很难对应准确，因为我们的尺寸是按照比例缩小的，如果除不尽，那么必然带来精度的缩减（为什么除的尽就不会带来精度的缩减呢？） 那么ROIAlign是如何规避这个问题的？ 其实ROIAlign使用的解决方案很直接：你不是会进行两次取舍吗？我不取舍行不行？直接用小数点表示行不行？等到最后Pooling的时候我用插值行不行？ ROIAlign确实也是这么做的，但是问题是： ROIAlign是如何实现用小数表征矩阵大小的？众所周知，我们的特征图本质上是一个矩阵，矩阵的尺度难道还能是28.8x28.6? 这个.6是啥意思？ 最后做Pooling，比如7x7的结果，那么小数的矩阵我怎么Pooling，用你的话说，我怎么做插值？ tf中实现ROIAlign细节及原理与其思考，不如从代码上直接来实现。实现上tf里面的 tf.image.crop_and_resize 方法几乎就实现了ROIAlign的步骤。我们看一个例子： 假如一个图片，a = tf.constant([[[1],[2]], [[3], [4]]])。 我们将其采样一个box，区域为：box = tf.constant([[0.2, 0.3, 0.2, 0.3]]). 你可能会感觉到奇怪，这个区域为什么是0.2， 0.3？到这里问题就出现了，此时我们必须要将图片想象成为一个连续的纸，而不是矩阵： 其实本质上，我们的图片仅仅只有四个像素值，但是你要让我求（0.2, 0.3) 这个坐标出的像素值，这不是为难我吗？既然我们索引不到任何值，那我们只能插值了。实际上当你用小数点去索取一个非整数值的时候，本质上你就是在让别人进行插值操作，为什么呢？因为你得创造新的像素点，而这个像素点像素值的计算只能通过插值进行. 那么插值是怎么求的呢？其实也很简单，现在x方向上求得插值，然后根据两个中间值，比如R1和R2，进行y方向求插值。 最常见的插值方式是线性插值. 上图详细的阐述了如何计算在矩阵下标不是整数时，如何求取对应的像素点值。 通过tensorflow代码来验证一下： a = tf.constant([[[1],[2]], [[3], [4]]])a = tf.expand_dims(a, axis=0)box = tf.constant([[0.2, 0.3, 0.2, 0.3]])box_ind = tf.constant([0])res = tf.image.crop_and_resize(a, box, box_indices=box_ind, crop_size=tf.constant([1,1])) 在代码中，首先a就是我们上面的四个像素的及其小的图片。box是我们要抠出来的区域，box_ind表示我们在第0个像素点进行扣，也就是扣取的位置。 最后你会发现res的计算结果跟我们计算的一致。 到这里你至少明白了两个道理： ROIAlign里面的非整数的像素值是如何求取的； 如何对结果进行插值计算的。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"卷积算法优化与winogra算法","slug":"2019_05_22_17_卷积算法优化与winogra算法","date":"2019-05-22T09:19:12.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/05/22/2019_05_22_17_卷积算法优化与winogra算法/","link":"","permalink":"http://yoursite.com/2019/05/22/2019_05_22_17_卷积算法优化与winogra算法/","excerpt":"本文介绍 卷积算法优化与winogra算法","text":"本文介绍 卷积算法优化与winogra算法 卷积算法优化与winogra算法 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 卷积算是视觉里面最基础的算法了.","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"pytorch自己实现一个CrossEntropy函数","slug":"2019_05_22_09_pytorch自己实现一个CrossEntropy函数","date":"2019-05-22T01:45:06.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/05/22/2019_05_22_09_pytorch自己实现一个CrossEntropy函数/","link":"","permalink":"http://yoursite.com/2019/05/22/2019_05_22_09_pytorch自己实现一个CrossEntropy函数/","excerpt":"本文介绍 pytorch自己实现一个CrossEntropy函数","text":"本文介绍 pytorch自己实现一个CrossEntropy函数 pytorch自己实现一个CrossEntropy函数 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 最近有社区朋友分享了一个问题, 面试某大厂Ailabs岗位时候, 对方要求在不借助任何外力的情况之下手写一个CrossEntropy. 这里不借助外力指的是, 不调用任何深度学习库, 当然可以用numpy, 不借助任何公式, 凭借记忆来写. 这还只是第一环, 据说通过率不足10%. 说明大家在仰望星空的时候也得脚踏实地呀. 今天就来传授一下如何编写一个CrossEntropy函数, 告诉大家这里面可能存在的一些坑. 理论(theory)编写之前, 先给大家补习一下理论, 相信很多人看到交叉熵, 想到的只有这个公式: $$ CrossEntropy = \\sum{(ylog(y’) + (1-y)log(1-y’))}$$ 写了一大堆, 没有保存…………………. 算了, 奇坑… 关于其实交叉熵可以通过一些简化得到最终结果. 举一个例子手算一下: [[1, 0, 0],[0, 1, 0]][[545, 54, 2],[232, 54, 546]] 两个样本,类别分别是0和1, 对应的是网络的输出. 那么如何计算交叉熵呢? 简单来说, 计算交叉熵分为两部: 先计算softmax, 然后计算log; 计算 $yi*log(yi)$ 首先大家可以踩一踩这个坑,如果计算softmax使得计算结果与pytorch一致: import torchimport numpy as np# label = torch.Tensor([# [1, 2, 3],# [0, 2, 1],# ])label = torch.Tensor([0, 2, 1]).long()fc_out = torch.Tensor([ [245, 13., 3.34], [45., 43., 37.], [1.22, 35.05, 1.23]])def one_hot(a, n): b = a.shape[0] c = np.zeros([b, n]) for i in range(b): c[i][a[i]] = 1 return np.array(c)def softmax(a): return [np.exp(i)/np.sum(np.exp(i)) for i in a]def cross_entropy_loss(out, label): # convert out to softmax probability out_list = out.numpy().tolist() out1 = softmax(out_list) print(out1) out2 = torch.softmax(out, 0) print(out2) # [0, 2, 1] -&gt; [[1, 0, 0], [0, 0, 1], [0, 1, 0]] # onehot label and rotate label_onehot = one_hot(label, 3) loss = np.sum(out1 * label_onehot.T) print(loss)loss = torch.nn.CrossEntropyLoss()lv = loss(fc_out, label)print(lv)lv = cross_entropy_loss(fc_out, label)print(lv) 我们一行代码实现的softmax, 实际上运算结果跟pytorch的softmax并不一致. 思考一下这是为什么?","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"卡尔曼滤波跟踪再再思考","slug":"2019_05_14_11_卡尔曼滤波再再思考","date":"2019-05-14T03:47:53.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/05/14/2019_05_14_11_卡尔曼滤波再再思考/","link":"","permalink":"http://yoursite.com/2019/05/14/2019_05_14_11_卡尔曼滤波再再思考/","excerpt":"本文介绍 卡尔曼跟踪滤波再再思考","text":"本文介绍 卡尔曼跟踪滤波再再思考 卡尔曼滤波跟踪再再思考 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 之前写过一篇关于卡尔曼滤波的文章，但是看起来很不直观。在这里再直观的理解一下。 深入理解卡尔曼滤波。其实在理解卡尔曼滤波之前，最好不要去想kalmanfilter，让我们先理解一下运动这个东西。 什么叫运动？一个球在二维平面滚来滚去，这个就叫做运动。一个点在一个一维的直线上前进或者后退，这个也是运动。那么什么是运动方程呢？ 我问一个问题，假如说，要你预测一个二维平面的小球，在下一个时间状态的位置，你需要知道哪些变量？有人说，我只要知道它当前时刻的位置以及当前的速度，不就知道了下一个时刻的位置了吗？是的没有错，这个想法是正确的，如果你知道了这些，那么，你也就是理解了卡尔曼滤波。 卡尔曼滤波用来估计运动状态其实就是刚才我们说的预测小球未知的问题。而其中的你需要知道的变量就是里面所讲的状态空间. 我们从最简单的一维运动来考虑率一下，一个小球围绕着一个圆弧运动，那么它是不是一维的呢？很显然，它其实就是掰弯了的直线，本身上是一维的，那么我们要描述它的运动，需要哪些物理量呢？ 很显然，我只要知道当前的角度，以及角速度，就可以了，也就是说，我只需要[θ，v], 就可以了。那么运动方程显然就可以这么写： $\\theta(t+1) = \\theta(t) + \\omega(t)*T$ $\\omega(t+1) =\\omega(t) (assuming avergae velocity)$ 显然，由于匀速的运动，我们的运动方程可以这样表示。注意了，这里面的核心其实就是两个： 你有多少个状态量才能确定状态空间； 你的状态量是怎么跟时间联系的。 后面大家做跟踪的时候就会遇到这个问题，为什么会有一个delta time？其实就是说白了，就是速度和路程的关系，小学学的？ 当我们要估计一些更加复杂的运动的时候，比如说，四个坐标的框，此时四个点都有速度，遇到这种问题，第一件事情就是先把运动方程写出来。然后就知道如何构建一个卡尔曼滤波器来进行状态的预测了。 OpenCV 卡尔曼滤波例子千说不如一站，一个简单的例子足可以让你完全明白这个操作。opencv使用卡尔曼滤波器，首先你需要明确，你的状态空间有哪些变量，然后有哪些变量是可以测量的（在上面的一维例子中，你的角速度其实是无法测量的，你不知道，你只知道当前的角度）。 第一件事情当然就是初始化一个卡尔曼滤波器，这个初始化需要初始化这些量： 状态转移矩阵，这个很简单，其实就是运动方程的矩阵提取出来，你能写出运动方程的矩阵形式就得到了状态转移矩阵，这个矩阵是常量（？）； 测量矩阵，就是你能拿到的实际值，如上面的角度你可以拿到但是角速度你不一定拿得到； 控制向量，一般是0； 过程噪声、控制噪声； 当前初始化的状态 代码： KalmanFilter KF(2, 1, 0);// 初始化转移矩阵KF.transitionMatrix = *(Mat_&lt;float&gt;(2, 2) &lt;&lt; 1, 1, 0, 1);// 这个转移矩阵其实就是上面的运动方程的矩阵形式，你会发现它其实就是一个常量// 状态，有两个，角度和角速度state.create(2, 1, CV_32F);// 测量噪声processNoise.create(2, 1, CV_32F);// 可测量的量measurement = Mat::zeros(1, 1, CV_32F);// ???这个测量矩阵是什么？setIdentity(KF.measurementMatrix);//系统过程噪声方差矩阵setIdentity(KF.processNoiseCov, Scalar::all(1e-5));//测量过程噪声方差矩阵setIdentity(KF.measurementNoiseCov, Scalar::all(1e-1));//后验错误估计协方差矩阵setIdentity(KF.errorCovPost, Scalar::all(1)); 跟踪里面KalmanFilter的作用到底是什么？长久以来，跟踪基本上都和kalmanfilter联系在一起，但是它在跟踪里面的作用到底是什么呢？我们梳理一下，一个跟踪器到底做了哪些事情： 新来的第一批检测，放到初始的tracks里面，第二次，新来的detection和tracks做匹配； 接着是核心一：匹配的度量和匹配的方式，比如你可以计算距离，也可以iou，到这里就要kalmanfilter, 你不能使用上一次的tracks的状态来做匹配，为什么？因为上一次跟这一次同一个物体运动了，位置变了，你需要通过卡尔曼滤波去预测这次的位置，再来匹配； 然后匹配过了，再用匹配后的真实值去更新对应trackid的kalman参数。 这样你就理解了，实际上在跟踪里面，跟踪本身的目标和kalmanfilter是相互依存的关系： 你滤波器为我们提供当前的估计状态，用来做匹配，匹配完了之后，我再帮助你更新. 说两点难点： 如果是跟踪点，也就是(x,y), 你可以计算欧拉距离，但是如果是4D，跟踪一个框，你还能用欧拉距离吗？ 马氏距离怎么用？或者用deep feature特征比对？但是这样也就没有kalmanfilter啥事儿了啊。 DeepSort跟踪算法的一些思想其实一个跟踪算法的好坏，跟滤波器的预测有一定的关系，但是归根结底，一定要是匹配足够准确, 所有匹配，就是你的新来的检测目标跟tracks进行配对，这个过程一定要足够的准确，现在我们一般用匈牙利算法来做匹配，也就是假设两两之间有一个cost，如何搭配使得他们的cost最低，从而让新来的检测目标尽可能的与tracks匹配正确。但是别忘了这里面有一个大前提，那就是你的匹配评价标准一定要足够的正确，如果你连评价都错了，那匹配自然也就一点儿不精准。 这里DeepSort里面使用了一种叫做 马氏空间距离 的评价指标。思考一下： 为什么不直接采用欧氏距离呢？*, 实际上，采用欧氏距离也是可以的，但是对于运动的目标，尤其是错综复杂在一起快速运动的目标，如果你仅仅采用欧氏距离，很有可能出现id switch的现象。那么这个马氏空间距离是否解决了这个问题？什么叫做马氏空间距离？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Apollo中点云3D障碍物分割的复现","slug":"2019_02_25_17_Apollo中点云3D障碍物分割的复现","date":"2019-02-25T09:37:30.000Z","updated":"2020-02-17T06:10:36.015Z","comments":true,"path":"2019/02/25/2019_02_25_17_Apollo中点云3D障碍物分割的复现/","link":"","permalink":"http://yoursite.com/2019/02/25/2019_02_25_17_Apollo中点云3D障碍物分割的复现/","excerpt":"本文介绍 Apollo中点云3D障碍物分割的复现","text":"本文介绍 Apollo中点云3D障碍物分割的复现 Apollo中点云3D障碍物分割的复现 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 简单的记录一下，Apollo中基于点云的3D障碍物分割复现过程。首先来看一下爱apollo的效果： 似乎视频加载不出来。 这个效果看上去非常不错。基于没有漏掉检测，怪不得Apollo说是全天候，不分昼夜的检测，这种看来还是非常就u有客观的使用场景的。简单的归纳一下Apollo中的检测思路： 首先根据高精度地图把不关心的区域去掉; 对剩下的点云进行分割，关于这个分割，目前来看输入网络的并不是原始点云，也不是像素的分割。而是将点云处理成为单元格，预测每一个单元格的属性，这些属性包括： 中心偏移 对象性 积极性 对象高度 接下来是后处理，很多参数是可以进行自定义。 MinBox的边框构建，关于MinBox的构建方法，很大程度依赖于聚类出来的点云的方向信息。 最后是基于匈牙利算法和鲁帮卡尔慢滤波的对象跟踪。 复现由于apollo使用的是caffe，基本上等于是没有开源，但是这里面多很思路值得借鉴和思考。比如，用分割的方法来拿到cell的基本属性，那么有了这些点，后续的边框构建和跟踪都很方便。分割器的输入为8个特征量，作为图像的8个通道： 1. 单元格中点的最大高度2. 单元格中最高点的强度3. 单元格中点的平均高度4. 单元格中点的平均强度5. 单元格中的点数6. 单元格中心相对于原点的角度7. 单元格中心与原点之间的距离8. 二进制值标示单元格是空还是被占用如 简单的分析一下，这些通道包含了这个点的基本信息，诸如最大高度，最高点强度，平均高度，平均强度等。这个本质我觉得是一个俯视图，只不过通道是8. 事实上，可以使用过滤无关障碍物之后的点云，也可以采用整个点云进行处理，网络似乎会基于所有点云进行构建。这里居然在训练的时候考虑了单元各距离原点的距离和方位信息，这些信息对于预测对象的位置有什么作用？ 对比一下现在的一些俯视图的方法，仅仅只是拼接了一个通道，比如只用强度，只用高度等。诸如PIXOR这样的方法不知道在进行俯视图拼接的时候用了哪些变量。接下来来看看输出是啥： 1. 中心偏移：center offset2. 对象性：objectness3. 是否是对象：positiveness4. 对象高度: object height 得到上面4个值之后，就可以对对象进行聚类了。这类的方法是有开源的，在这里。聚类的时候，基于中心偏移预测构建有向图，采用压缩的联合查找算法，基于是否有对象来构建障碍物聚类。 这里，红色的箭头表示的是单元格对象中心偏移预测，这里的中心便宜可以理解为方向，蓝色区域表示objectness在0.5以上的单元格，打概率表示这里会有物体。最终你会发现预测的中心点偏移会指向一个“真正的中心”，从而就构成一个聚类。 接下来就是后期处理了，接下来就是处理上面的聚类结果，每个聚类结果对象都由若干个单元各组成，有一点无法解释的是，他是如何得到物体类别的呢？在这里我们似乎无法得到物体的类别？比较难。这里类别的判断似乎是根据高度来的？ 接着就是边框的构建了，通过刘变形进行构建。六边行的方案确实是一个比较好的方案。 接下来是基于匈牙利算法的跟踪，这篇博客写的不错：波尔 最后总结一下：由于这个方法未完全开源，也许我们能做的，只能把caffemodel更新，先跑通一下，在那个基础之上，再进行更新。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"2019新的一年目标","slug":"2019_02_11_14_2019新的一年目标","date":"2019-02-11T06:25:50.000Z","updated":"2020-02-17T06:01:58.934Z","comments":true,"path":"2019/02/11/2019_02_11_14_2019新的一年目标/","link":"","permalink":"http://yoursite.com/2019/02/11/2019_02_11_14_2019新的一年目标/","excerpt":"本文介绍 2019新的一年目标","text":"本文介绍 2019新的一年目标 2019新的一年目标 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 简单的描述一下2019年技术目标。新的一年需要在已有的基础上进一步增进一些收获。2019年已经不能单纯的追求技术上带来的成就，应当进一步追求管理、市场上的成就。新年总目标可以在分为上半年和下半年两个时间段来完成。 上半年: 这部分时间其实不会很长，从2月份开始到6月份底，只有5个月的时间，这5个月可以做的事情有很多，需要完成的硬性指标也很多。 工作 感知模块从1.0到2.0提升，在红绿灯检测、地面分割的基础上，进一步完成并实现3D物体的检测和跟踪，这部分工作细化为两个方面，一个是完善地面障碍物投影部分的工作，另外一个是开创一个新的自有的3D障碍物检测算法流程，并在此基础上完成跟踪的工作，这部分工作分别需要在2月份和三月份完成; 随着2019年，固态激光雷达的普及，毫无疑问，点云算法将会是实际场景下目标检测应用的关键一环。而固态激光雷达将会以图像级别的点云图带来前所未有的高分辨率感知数据。需要完成一套精准的，基于RGBD（未来的点云数据可能就是RGBD）的3D目标检测算法和方案，实现端到端的3D目标检测，自有高精度高速度的3D目标感知算法需在4月份完成。 完成至少从0到1的AI算法两个，如FasterRCNN，SSD; 基于单目的SLAM完成一个小型的室内机器人设计，开始探索cv在slam的应用。 副业 开始学习影视后期剪辑作为业余爱好; 继续完善奇异AI网站，继续为用户提供开箱即用的AI算法; 以奇异AI之名，至少完成与一个B端客户的合作，深入探索AI算法外包的用武之地; 着手录制一个系列《奇异异视界：一个教你如何玩转高科技的频道》。从AI入门开始，来完成一个个的小挑战。视频讲解; 完成Setu产品的后续研发，使之成为一个完善的社交产品 下半年: 在新的岗位上开始新的航程; 开始买房踩点，出一份深圳购房报告，分析地理位置，价格，进行周密的比对;","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"KalmanFilter再思考","slug":"2019_01_28_10_KalmanFilter再思考","date":"2019-01-28T02:19:39.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/01/28/2019_01_28_10_KalmanFilter再思考/","link":"","permalink":"http://yoursite.com/2019/01/28/2019_01_28_10_KalmanFilter再思考/","excerpt":"本文介绍 KalmanFilter再思考","text":"本文介绍 KalmanFilter再思考 KalmanFilter再思考 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 卡尔慢滤波操作，其实本质上卡尔慢滤波并不是一个滤波器，而是一个信息融合算法。简单来说，根据测量值来预测更加准确的下一个状态。这个在多目标跟踪领域很有用，通过预测上一帧的bboxes在下一帧的坐标，根据检测的ground truth作为测量值，进行卡尔慢滤波融合，可以得到上下帧bboxes之间的对应关系，从而实现多目标的跟踪。 kalmanfilter的简单表达式最简单的kalmanfilter的形态，是通过一个非常简单的表达式来表达的：$$\\hat{X}_k = K_k \\cdot z_k + (1 - Kk) \\hat{X}{k-1}$$这是一个简单的公式，但是基本上蕴含了一个完整的卡尔慢滤波思想。$k$其实表示的意思是第$k$个时刻，那么在$k$时刻的状态，取决于两个因素，一个是$z_k$， 这个就是$k$时刻的测量值，这个值不一定准确，存在误差，$Kk$这个是卡尔慢增益，每一个时刻都存在一个最优值，是需要我们去优化的量。而 $X{k-1}$是上一个时刻的状态。因此，要计算出当前时刻的状态，那么只要知道测量值和上一个时刻的状态，就可以得到。那么问题来了。如何计算这个卡尔慢增益呢？ 此时需要看一下经典的卡尔慢滤波两个方程，这是求取卡尔慢增益的关键：$$xk = Ax{k-1} + Buk + W{k-1} \\z_k = Hx_k + v_k$$ 这两个公式的解释是这样的： 1): 第一个公式告诉我们，当前时刻的状态，取决与上一个时刻状态以及一个控制参数，加上噪声，通常情况下这个控制参数可以没有; 2): 第二个公式告诉我们，当前时刻的测量值，由当前的状态，及测量噪声值确定，默认该值应当服从正太分布。 这两个经典公式，是求解卡尔慢增益的关键方程。 上面其实仅仅适合于线性的情况，而对于非线性的情况，则有着很大的不同，此时状态表达式应该表达为：$$xk = f(x{k-1}, u_k) + w_k \\y_k = h(x_k) + v_k$$此时就是一个非线性的形式，由于非线性的存在，你会发现，假如x是正太分布，但是i下一个时刻就不是正太分布了，因此整个链条就断裂了，此时就需要使用EKF来进行计算，也叫做扩展卡尔慢滤波。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"FasterRCNN Anchor机制的再思考","slug":"2019_01_23_14_FasterRCNN_Anchor机制的再思考","date":"2019-01-23T06:09:25.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/01/23/2019_01_23_14_FasterRCNN_Anchor机制的再思考/","link":"","permalink":"http://yoursite.com/2019/01/23/2019_01_23_14_FasterRCNN_Anchor机制的再思考/","excerpt":"本文介绍 FasterRCNN Anchor机制的再思考","text":"本文介绍 FasterRCNN Anchor机制的再思考 FasterRCNN Anchor机制的再思考 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu anchor都知道，但是anchor的生成方式以及细节很多人不知道，为什么呢？因为很多人都知道 [16, 128, 256], [0.5, 1, 2], 根据anchor scale和ratio来确定9个anchor，但是这个9个anchor到底是怎么表示的？是长宽，还是四个点的坐标？以及为什么你看到的图，很多 图都是 跟你设想的不一样的？（按道理地一个尺寸3个框分别是 16x8, 16x16, 8x16, 这3个框放到同一个中心点，肯定不是你想的或者你大多数时候看到的那样子）。 anchor真正的生成与scale的根号有关实际上anchor并不是这样生成的，它跟一个根号有关。这个根号，其实就是保证长宽比，同时长宽跟base_size有个区别。先梳理一下算法吧，然后在来看代码： 算法步骤： 首先，你要有base_size, 所有base size，其实就是某个尺寸的感受野，比如你的的featuremap尺寸是 32,你就要基于32生成anchor，然后你要有比例和scale，这个scale呢，实际上是根据ratio计算的，比如你ratio是0.5,再基础尺寸上缩小一半，那就是8; 然后你需要根据 8, 16, 32 3个scale，依次生成各自得到3个基础w和h，关键的点就在这里了, 比如在scale 8的情况下，高度和宽度应该是多少，按照 scale x sqrt(0.5) 和 scale x sqrt(2) 分别计算。这里每一步计算的结果可以看一下： scale: 8, ratio: 0.5, w: 90.50966799187809, h: 181.01933598375618scale: 8, ratio: 1, w: 128.0, h: 128.0scale: 8, ratio: 2, w: 181.01933598375618, h: 90.50966799187809scale: 16, ratio: 0.5, w: 181.01933598375618, h: 362.03867196751236scale: 16, ratio: 1, w: 256.0, h: 256.0scale: 16, ratio: 2, w: 362.03867196751236, h: 181.01933598375618scale: 32, ratio: 0.5, w: 362.03867196751236, h: 724.0773439350247scale: 32, ratio: 1, w: 512.0, h: 512.0scale: 32, ratio: 2, w: 724.0773439350247, h: 362.03867196751236 可以看出，scale是8,但是实际上并不是在16的基础上乘以8,实际上比16乘以8更大，变化范围ie在 [90, 181], 这啥意思？为什么要在16x16的基础上，再扩张成那么大的尺寸？其实重点是 最终生成的anchor是想对于原图的坐标点位置. 着重需要理解的事，假如生成的anchor是想对于原图的坐标位置，那如何与groudtruth结合起来？如何参与到网络的反响传播中去？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比","slug":"2019_01_09_11_MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比","date":"2019-01-09T03:15:19.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/01/09/2019_01_09_11_MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比/","link":"","permalink":"http://yoursite.com/2019/01/09/2019_01_09_11_MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比/","excerpt":"本文介绍 MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比","text":"本文介绍 MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比 MobileNetV2,DenseNet,ShuffleNet几种轻量网络性能对比 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 这篇文章简要的对比一下常用的轻量网络之间的性能差别。在本文发表的时候，ShuffelNet都已经有V2版本了。总的来说，不对比Mobilenetv1, shuffelnet是在mobilenetv1版本上的改进，其实就是添加了一个group convolution，而Mobilenetv2是在v1的版本基础之上，先从网络结构加速的MobileNetv1开始说起。 MobileNetV1在v1,版本，最简单直接的两个操作就是深度优化的卷积滤波器，以及点优化的卷积，听起来牛逼，实际上insights就两个： depth-wise 的filter，以前都是直接进行卷积运算，对每一个通道进行比如3x3卷积运算，现在是先进行resize，先降低尺度，然后在使用1x1卷积进行通道扩充。 1x1卷积的提出，说白了就是扩充通道，不改变尺度。 好像就这两个东西，那么v2版本有啥提升呢？先不说v2,根据网络的前后顺序，接下来说一下shufflenet。 ShuffleNetV1其实shuffelnetv1说实话，就是在mobilenetv1基础上，加了一个group操作。关于这个Group卷积，在之前我有过一篇文章详细论述。其实就是将整个channel，拆分为几组，分别进行卷及计算，而ShuffleNet在group之间进行了一个打乱的操作，提升了特征的融合性能。 MobileNetV2在MobileNetV2中，最重要的就是反向残差，Inverted Residual, 这个是在v1中没有的，另外一个是线性瓶颈，Linear Bottlenecks, 对于反向残差，其实没有设么好理解的，就是将ResidualNet中的残差连接方式，进行了反向。线性的瓶颈，这个线性瓶颈到底起来什么作用，其实有点像采用这个东西，代替了ReLU。此处似乎需要思考一个问题：为什么卷积要用ReLU 作为激活函数？为什么呢? ShuffleNet V2这是ShuffleNet的第二个版本。这个论文分析了，通过FLOPS来衡量一个网络计算速度的指标是不可学得，原因是，卷积运算在不同的平台都有不同的优化的，比如CUDA可以并行计算多个卷积操作，其中这个FLOPS还只是包含了浮点数的计算时间，类似的内存访问损失似乎没有加入进来。于是这篇论文给出了设计高速轻量网络的几条guidelines： 卷积操作，输入输出channel一致可以使得内存访问成本最低; 过度的群卷积会增加mac访问成本; 网络的碎片化会降低网络的并行度; 元素级运算，比如add，relu，虽然其flops几乎没有，但是它的内存访问时间不忽略 然后根据以上四条准则，设计除了shufflenetv2的结构： 也就是做到：1) 保证每个conv输入channels和输出channels数量一致，没有使用群卷积，采用channels split操作，最后连接没有采用add和relu，而是concat，降低mac消耗。可以看到ShuffleNetV2在速度和精度上，似乎都比MobileNetV2要强。尤其是在ARM上的运行速度，很快。 总结最后总结一下，这些轻量级的网络中，似乎最后ShuffleNetV2是最先进的，它几乎融合了各家特长，并且将速度做到了极致。有时间得把ShuffelNetV2用起来，看看实际应用场景下的表现如何。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"超分辨率SuperResolution手记一","slug":"2019_01_03_14_超分辨率SuperResolution手记一","date":"2019-01-03T06:16:53.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/01/03/2019_01_03_14_超分辨率SuperResolution手记一/","link":"","permalink":"http://yoursite.com/2019/01/03/2019_01_03_14_超分辨率SuperResolution手记一/","excerpt":"本文介绍 超分辨率SuperResolution手记一","text":"本文介绍 超分辨率SuperResolution手记一 超分辨率SuperResolution手记一 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 挖个坑，有时间研究一下，这玩意是一个很好的应用场景。并且，许多场合可能会用到。比如重构超高的分辨率图像，甚至是在数据集构建场合。目前来讲，一些已有的方法有 fsrcnn， SRCNN等，输入数据其实就是lr和sr的pair。还有基于视频的分辨率重建方法，VESPCN，这些方法都是出自与英国的一个创业团队，该团队已经被twitter收购。 最牛逼的好像是SRGAN，最新的，通过GAN网络构建分辨率重建的你网络。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"从shuffleseg以及分组卷积说起","slug":"2019_01_03_09_从shuffleseg以及分组卷积说起","date":"2019-01-03T01:27:47.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2019/01/03/2019_01_03_09_从shuffleseg以及分组卷积说起/","link":"","permalink":"http://yoursite.com/2019/01/03/2019_01_03_09_从shuffleseg以及分组卷积说起/","excerpt":"本文介绍 从shuffleseg以及分组卷积说起","text":"本文介绍 从shuffleseg以及分组卷积说起 从shuffleseg以及分组（群）卷积说起 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 什么是shufflenetshufflenet两个核心的骚操作其实就是：逐点的群卷积与通道混洗，in english it is: pointwise group convolution and channel shuffle, so this is how the name comes. 在引入shufflenet的分组卷积之前，有必要总结一下当前在加速卷积运算基础上做的一些变种研究： mobilenetv2: 可分离卷积，几乎所有的卷积改进算法都是降低 mxm 的运算复杂度的，比如你 mxm, 我用 (mxa + mxb), 可以实现相同的功能，但是计算更小。为什么呢？举个例子，输入是16通道，用3x3的卷积将输出变为32通道，这个操作可以用32个3x3的卷积完成，但是，这样的操作参数对应为： 16x32x3x3 (32个3x3卷积对16尺寸的输入做卷积)，我换一种方式，先用 1个3x3的卷积操作，此时尺寸满足要求了，但是通道是16,再用32个1x1卷积去卷积它，最终也可以得到，但是此时的参数数目为（16x3x3 + 16x32x1x1), 参数数目确实减少了很多。 逐点卷积，其实就是用1x1的卷积去改变通道，在不改变尺寸的情况下，这与上面的第二部是一样的 最后，sufflenet里面的群卷积是个什么东西？其实shufflenet并没有提出群卷积，而是提出了一个解决群卷积问题的方法，那么群卷积存在什么问题呢？Group Convolution 其实并不是什么新玩意儿，早在alexenet里面就已经存在，简单来说，就是将同一个层的featuremap，斩断，分为多个group，每个group单独的再进行卷积，然后结果concat，这样你会发现一个有趣的现象，最终的featuremap会存在分截，比如，第一个group全是黑白的，第二个group全是彩色的。 shufflenet觉得这是一个问题，看起来好像确实是一个问题，问题在哪儿呢？输出的不同group之间信息无法共享了。而shufflenet就提出了这么一个操作，使得输出可以很好的共享信息： 总的来说，shufflenet总结出来了几个结论： 有组的比没有组的好，因为有组可以让组学习到不同的通道特征; shuffle组的比没有shuffle组的要好，因为shuffle之后每个组的信息将不再单一，使得捕获到的信息更加丰富 shuffleseg有人基于shuffelnet测试了，基于shufflenet基础上，构建分割算法。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"ROS武功秘籍","slug":"2018_12_02_17_ROS武功秘籍","date":"2018-12-02T09:21:12.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/12/02/2018_12_02_17_ROS武功秘籍/","link":"","permalink":"http://yoursite.com/2018/12/02/2018_12_02_17_ROS武功秘籍/","excerpt":"本文介绍 ROS武功秘籍","text":"本文介绍 ROS武功秘籍 ROS武功秘籍 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 这是ros的一系列记录，其中包括，从0到自己做一个仿真机器人的所有流程。包括自己录制地图，使用地图，进行导航，机器人控制，局部规划，全局规划，避障算法等一系列应用。 导航导航部分包含了整个机器人核心模块，没有导航机器人无法走，没有导航，你的避障算法在牛逼也没有用武之地。但是此时此刻，我们需要掌握的并不是直接到导航这一步。而是见地图。那么问题来了，这地图要怎么建呢？这里的地图有两个，一个是2D的地图，一个是3D的点云地图（高精度地图），我们先看看使用很广泛的2D地图是如何建立起来的。 首先安装相应的依赖 cd ~/catkin_ws/src/git clone https://github.com/ros-perception/openslam_gmappinggit clone https://github.com/ros-perception/slam_gmapping.gitgit clone https://github.com/ros-planning/navigation.gitgit clone https://github.com/ros/geometry2.gitgit clone https://github.com/ros-planning/navigation_msgs.gitcd ~/catkin_ws &amp;&amp; catkin_make 此时，你所需要的slam_gmapping 等包就具备了。接下来要做什么呢？ 启动仿真机器人，我们实时的录一段地图 接下来，得实时的来录制一段地图了。我们要玩的东西，就是turtlebot3,一个基于gazebo的仿真机器人。 git clone https://github.com/ROBOTIS-GIT/turtlebot3.gitgit clone https://github.com/ROBOTIS-GIT/turtlebot3_machine_learninggit clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.gitgit clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations 所有的东西就安装完了，此时此刻，我们需要启动一下世界里面的gazebo机器人。 # add robot model to zshrcecho &apos;export TURTLEBOT3_MODEL=waffle&apos; &gt;&gt; ~/.zshrcroslaunch turtlebot3_gazebo turtlebot3_world.launch","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"ndt配准，建图，与无人车定位","slug":"2018_11_29_18_ndt配准，建图，与无人车定位","date":"2018-11-29T10:04:44.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/11/29/2018_11_29_18_ndt配准，建图，与无人车定位/","link":"","permalink":"http://yoursite.com/2018/11/29/2018_11_29_18_ndt配准，建图，与无人车定位/","excerpt":"本文介绍 ndt配准，建图，与无人车定位","text":"本文介绍 ndt配准，建图，与无人车定位 ndt配准，建图，与无人车定位 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 无人车系统中的定位问题不管是机器人还是无人车，机器人所在的位置，程序是无法知道的。为什么？很简单，因为我无法用GPS或者其他传感器提供定位信息，一句话，不够准确啊。但是我们的车，机器人又需要cm级别的定位精度，这就有了一个定位问题。车走到哪里，程序不知道，这就造成了路径规划的困扰，因此要想进行控制，路径规划，就必须要先知道机器人的位置，这个操作使用定位来解决。NDT便是一种定位的方法。简单的说一下NDT这个玩意儿的流程是什么。假如我有了高精度地图，有了一个frame的雷达扫描点云，通过配准（也就是匹配）就可以知道当前我的车在地图中的位置。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"从为什么VOC的分割标注8位单通道确实彩色图片说起","slug":"2018_11_13_16_从为什么VOC的分割标注8位单通道确实彩色图片说起","date":"2018-11-13T08:35:41.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/11/13/2018_11_13_16_从为什么VOC的分割标注8位单通道确实彩色图片说起/","link":"","permalink":"http://yoursite.com/2018/11/13/2018_11_13_16_从为什么VOC的分割标注8位单通道确实彩色图片说起/","excerpt":"本文介绍 从为什么VOC的分割标注8位单通道确实彩色图片说起","text":"本文介绍 从为什么VOC的分割标注8位单通道确实彩色图片说起 从为什么VOC的分割标注8位单通道确实彩色图片说起 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 记录一个很基础的问题，却从来没有人关心过。甚至很多人不懂。但是这个问题却不是一个简单的问题，如果你没有弄懂这个问题，下面两个问题你很难回答： 用opencv读取voc的分割label，能获取到以class id标注的像素点label吗？如果能，怎么才能实现？另外，就是这个问题，为什么8位单通道是彩色的？ 十分重要的数据预处理很多人凭借着对网络的深入理解，就可以对网络的运作方式把控与胸，但实际上并没有这么简单。不同的预处理操作，可能会带来完全不同的结果，这就像给图片加个噪点，网络就把熊猫变成大猩猩一样。 通常，VOC的标注是8bit的，请注意，是8位的，这个8位怎么理解呢？我们正常的图片是24bit，也就是3×8, 3个通道的8位，每个通道的颜色值范围是0-255.因此你的一张图片矩阵应该是： [ [[233, 221, 123], [123, 34, 44], ...] ] 但是呢，只有一个通道的8位，是u如何实现彩色的呢？为什么一个通道就能实现彩色，我还要3通道做什么呢？ 这个问题我也不懂。 但是我想找的是，这种8位的彩色图为什么是彩色的？ 结论：并非只有3通道，它才是彩色的，彩色只是一种格式，与矩阵通道数没有太大的关系。灰度图也是可以转换成彩色的。 opencv如何读取伪彩色图VOC的数据用PIL可以读取，但我就想用opencv读，能不能实现？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"点云与图像融合的3D物体检测","slug":"2018_11_06_09_点云与图像融合的3D物体检测","date":"2018-11-06T01:45:32.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/11/06/2018_11_06_09_点云与图像融合的3D物体检测/","link":"","permalink":"http://yoursite.com/2018/11/06/2018_11_06_09_点云与图像融合的3D物体检测/","excerpt":"本文介绍 点云与图像融合的3D物体检测","text":"本文介绍 点云与图像融合的3D物体检测 点云与图像融合的3D物体检测 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 做3D检测可以采用纯图像，纯点云，也可以融合点云和图像，毫无疑问，后者是最复杂的。现有的一些比如MultiView或者AVOD都是将点云提取出更多的调整图作为输入，来得到网络的input。比如在MultiView方法中，就采用了 height maps, density map, intensity map 作为输入 height maps: 俯视图下，每个点的最大高度，有点像分布图？？？ density: 密度图，每个像素范围内的点云密度信息，每个像素也就是 0.1m*0.1m的区域（这个密度是计算面积还是体积呢？） intensity: 强度，指的是反射强度。我们知道点云每个点信息其实是 xyzi的，除了3D坐标以外，最后一个值就是强度信息。 这样，杂乱无章的3D点云信息就转换成了具有物理意义的2D图像输入。 所以在这里就有一个问题需要解决了，如何将原始的激光点云数据转换成相应的height maps, density, intensity maps. 这个很重要。 AVODAVOD 方法，目前实际测试的时候会有一个偏左的可以检测到，稍微右一点的无发检测的问题。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"深度可分离卷积的具体理解：MobileNetV2设计灵感","slug":"2018_10_27_15_深度可分离卷积的具体理解：MobileNetV2设计灵感","date":"2018-10-27T07:19:01.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/10/27/2018_10_27_15_深度可分离卷积的具体理解：MobileNetV2设计灵感/","link":"","permalink":"http://yoursite.com/2018/10/27/2018_10_27_15_深度可分离卷积的具体理解：MobileNetV2设计灵感/","excerpt":"本文介绍 深度可分离卷积的具体理解：MobileNetV2设计灵感","text":"本文介绍 深度可分离卷积的具体理解：MobileNetV2设计灵感 深度可分离卷积的具体理解：MobileNetV2设计灵感 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Depthwise separable convolution在对比不同网络的结构的时候，很多网络它背后的设计思想却很容易被人忽视。其中MobileNet中使用的深度可分离的卷积结构就比较难以理解。 简单来说，就是，为什么这种结构被人认为是可以减少计算量的？它和直接的卷积结构有和区别？需要为它自定义单独的层去实现吗？ 对比两个例子： 假设有一个3x3大小的卷积层，输入通道是16, 输出通道是32,那么既然输出通道是32, 就可以使用32个3x3的卷积核去遍历输入，这里面要用到的参数是 16x32x3x3=4608 个 上面是通常的卷积运算方式。但是如果将里面的操作分为两部进行，也同时达到需要的输出维度可以这么做： 先用 16个 3x3 的卷积核遍历16个通道里面的数据，得到16个特征图谱，在用32个1x1的卷积核去遍历这16个特征图谱，进行相加融合，这个过程使用的参数是 16x3x3 + 16x32x1x1 = 656 个参数。 同样是使用的通道变化了，为什么后面这个操作所用到的参数更少了呢？？第一种方式也就是常规方法是直接一步到位，用输出channel以及对应的卷积核去操作，那结果自然也是一步到位。而第二种方法很巧妙的使用了两步计算，第一步先把数据遍历一遍，尺寸变换一下; 接着第二部用了1x1卷积的特性，不改变形状之改变通道，从而通道也变化了。 这样的方式，便是将传统的卷积分为了两步计算，但是时间是原来的1/7,同时参数是原来的1/9. 可以说是非常成功的trick了。 看看使用Pytorch如何实现它呢？ class InvertedResidual(nn.Module): def __init__(self, x, output, stride, expand_ratio): \"\"\" this is the core of MobileNet, something just like ResNet but not very much alike. it is called Inverted Residual, the opposite residual operation, how does it operation anyway? Only when stride == 1 &amp;&amp; input == output, using residual connect other wise normal convolution what does this expand_ratio for? this value is the middle expand ratio when you transfer input channel to output channel ( you will get a middle value right? so there it is) :param x: :param output: :param stride: :param expand_ratio: \"\"\" super(InvertedResidual, self).__init__() self.stride = stride assert stride in [1, 2], 'InsertedResidual stride must be 1 or 2, can not be changed' self.user_res_connect = self.stride == 1 and x == output # this convolution is the what we called Depth wise separable convolution # consist of pw and dw process, which is transfer channel and transfer shape in 2 steps self.conv = nn.Sequential( # pw nn.Conv2d(x, x * expand_ratio, 1, 1, 0, bias=False), nn.BatchNorm2d(x * expand_ratio), nn.ReLU6(inplace=True), # dw nn.Conv2d(x * expand_ratio, x * expand_ratio, 3, stride, 1, groups=x*expand_ratio, bias=False), nn.BatchNorm2d(x*expand_ratio), nn.ReLU6(inplace=True), # pw linear nn.Conv2d(x*expand_ratio, output, 1, 1, 0, bias=False), nn.BatchNorm2d(output), ) def forward(self, x): if self.user_res_connect: return x + self.conv(x) else: return self.conv(x) 其实十分简单。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"目标检测的前世今身","slug":"2018_10_25_15_目标检测的前世今身","date":"2018-10-25T07:57:02.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/10/25/2018_10_25_15_目标检测的前世今身/","link":"","permalink":"http://yoursite.com/2018/10/25/2018_10_25_15_目标检测的前世今身/","excerpt":"本文介绍 目标检测的前世今身","text":"本文介绍 目标检测的前世今身 目标检测的前世今身 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Faster-RCNN算法再思考实际上2阶段检测算法从现在看来，已经有点过时，但是不失为一种有效的检测方法。对比单阶段检测算法来说，毫无疑问，其准确率和精度都是可以得到保证的。但是在速度上却不然。 fastercnn的运算过程可以分为两大快，一块是RPN，在特征图上提取感兴趣区域，所谓的感兴趣区域就是可能包含物体的对应的特征图，比如一张图片最后出来的特征图大小是 512x38x56 , 那么感兴趣区域就是从这张featuremap中提取。最后你会发现每个感兴趣区域都会有一个 rpn_cls (这个类别只有两个，is_object or not), 然后会有相应的概率，这里的疑问是 是否会预测框的坐标？。 然后这个rpn网络的输出再次输入到两个分支，一个预测类别，一个预测box的微调坐标。 每次说检测都tm说不清楚，必须要通过代码来解释。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"行人轨迹预测","slug":"2018_10_16_10_行人轨迹预测","date":"2018-10-16T02:38:00.000Z","updated":"2020-02-17T06:01:58.933Z","comments":true,"path":"2018/10/16/2018_10_16_10_行人轨迹预测/","link":"","permalink":"http://yoursite.com/2018/10/16/2018_10_16_10_行人轨迹预测/","excerpt":"本文介绍 行人轨迹预测","text":"本文介绍 行人轨迹预测 行人轨迹预测 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 开始记录一下行人轨迹预测相关的工作。思考行人轨迹预测问题的时候，我想应该分为两个方面： 基于跟踪的轨迹预测，跟踪一个目标，然后把轨迹画出来 基于检测的轨迹预测，每一针率把目标检测出来，然后对目标进行轨迹绘制，从而得到他的行进方向。 这个两个方面都可以实现轨迹的预测，但是他们二者却有这本质的区别。跟踪的方法其实无法离开检测的，跟踪的好处是能够区分目标，知道当前帧的目标在下一帧的时候在哪个地方。但是检测却无法做到这一点。因此必须先实现跟踪，才能做轨迹的预测。 行人检测任务可以细分为两个点：1. 实现行人的ID匹配，2. 实现轨迹的行走方向预测。 Some work OpenPTrack 记录一些已经有的work，在这方面，可能OpenPTrack是做的十分好的。它提供了包括Yolo在内的行人检测，跟踪，人体姿态检测在内的许多功能。网址：http://openptrack.org/。 新版的OpenPTrack v2 叫做 Gnocchi. 不过在这个工作中，貌似对传感器的要求是需要一个kinect，但是这种东西要kinect肯定是十分复杂的。 OPT2需要RGBD的图像进行3维的定位和跟踪，事实证明这种方式目前还是过于复杂。简单的多目标追踪应该就足以应付许多复杂的场景和情况。 Others 思路实现一个行人轨迹预测，需要两个东西，一个高性能的检测器，一个高性能的跟踪器，最后二者结合的轨迹预测。 先实现一个高性能的检测器？","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"","slug":"2020_02_16_工作第二年开工在即有感","date":"1970-01-01T00:00:00.000Z","updated":"2020-02-17T06:11:25.004Z","comments":true,"path":"1970/01/01/2020_02_16_工作第二年开工在即有感/","link":"","permalink":"http://yoursite.com/1970/01/01/2020_02_16_工作第二年开工在即有感/","excerpt":"","text":"疫情年开年随想散文这是一篇天马行空的随想散文，我所租住的公寓断网，闲来无事，遂开始码字，写下这一篇随想，等到来年再来看时，看是否会增加一些感悟。2020注定是不同寻常的年份，一场突如其来的疫情席卷了整个中华大地：昔日繁华的武汉成为现实版的人间地狱，各地的人们纷纷关闭门户，唯恐那不长眼的病毒从什么地方钻进屋子，将一家人的性命悄无声息的带走。 我是2.7日回深圳的，这是一座繁华的都市，而如今却也萧条的惊人。离家那日，父亲、母亲和弟弟，都来送我，虽然病毒闹得厉害，却也丝毫没有减退家人送我的热情。我在机场打印了登机牌，便让他们不要在机场久留，早点开车回去。此时的昌北机场，只有零零散散的乘客和几位工作人员，丝毫不像年后返程的拥挤景象，望着家人远去的背影，不由得感叹一声：这一生太短，人生就像这疫情一样无常，还没有好好珍惜，它就离你远去了。我紧了紧双肩背包的背带，踏起步子就向前走去。 这几日独自一人在深圳居住，由于疫情隔离的紧，企业并不敢早早复工，于是我一人做饭一人食，也不敢出门，感受着这安静的岁月，要说陪伴我的，或许只有我这台电脑吧。经历了这几天的独自生活，才明白，一个人生活确实不容易，尤其是当你没有人陪你说话的时候，内心的幽闭恐惧症就暴露出来了。不过好在，我发现了一个很好的方法来抵御这种孤独：写作。多少年来，记录人类辉煌文明历史的不就是文字吗？人们从文字中感悟，从文字所描绘的世界里获得感同身受的感觉，从文字所描写的人物中感受每一位角色的性格、谈吐，就仿佛这个人就生活在你身边，这便是文字的魅力。难怪我们身边的年轻人们，男人喜欢看历史小说，女人喜欢看言情小说，都逃不过这文字所塑造的神奇魅力。我不由得想起来刘慈欣所著的三体小说，其充满想象力的描写、扣人心魄的情节，让每一位读者都将自己的命运和宇宙的命运紧密的联系在了一起。每次慨叹大刘的想象力，他所创作的文学作品，大气恢弘，并且属于非常硬的科幻类小说，这也是我非常喜欢的一个类型。作为一个科技工作者，就科幻而言，那些有可能实现的科幻类想象才是更令人心动的和遐想的。 明日就将上班，也许是时候出去窥探一下灾后的社会了。","categories":[],"tags":[]}]}